{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function random iri parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def generate_parts(target_value, num_parts, tolerance=0.01):\n",
    "    total_sum = target_value * num_parts\n",
    "    while True:\n",
    "        # Generate random parts\n",
    "        parts = np.random.uniform(low=total_sum / num_parts * 0.9, high=total_sum / num_parts * 1.1, size=num_parts)\n",
    "        # Ensure the sum is correct\n",
    "        if np.abs(np.sum(parts) - total_sum) < tolerance:\n",
    "            return parts\n",
    "\n",
    "target_value = 4.182634\n",
    "num_parts = 4\n",
    "\n",
    "# Generate parts\n",
    "parts = generate_parts(target_value, num_parts)\n",
    "\n",
    "# Create DataFrame\n",
    "data = {\n",
    "    'old_value': [target_value] * num_parts,\n",
    "    'iri_random': parts\n",
    "}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert iri 20 to 5 and random iri in 5m and join iri and rut all .csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated xw_iri_qgis_20240726RUN01.csv into IRI DataFrame.\n",
      "Updated xw_rutting_20240726RUN01.csv into Rutting DataFrame.\n",
      "Updated xw_iri_qgis_20240726RUN02.csv into IRI DataFrame.\n",
      "Updated xw_rutting_20240726RUN02.csv into Rutting DataFrame.\n",
      "Updated xw_iri_qgis_20240726RUN03.csv into IRI DataFrame.\n",
      "Updated xw_rutting_20240726RUN03.csv into Rutting DataFrame.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Define the path to your directories\n",
    "path = r\"D:\\xenomatixs\\output\\survey_data_20240726\\Output\"\n",
    "pic = r\"D:\\xenomatixs\\output\\survey_data_20240726\\PAVE\"\n",
    "\n",
    "def split_and_randomize(value, parts=4):\n",
    "    random_parts = np.random.rand(parts)\n",
    "    random_parts /= random_parts.sum()  # Normalize to ensure sum is equal to 1\n",
    "    split_values = random_parts * value  # Scale fractions to match the original value\n",
    "    return split_values\n",
    "\n",
    "def generate_parts(target_values, num_parts, tolerance):\n",
    "    parts_list = []\n",
    "    for target_value in target_values:\n",
    "        total_sum = target_value * num_parts\n",
    "        while True:\n",
    "            # Generate random parts\n",
    "            parts = np.random.uniform(low=total_sum / num_parts * 0.9, high=total_sum / num_parts * 1.1, size=num_parts)\n",
    "            # Ensure the sum is correct\n",
    "            if np.abs(np.sum(parts) - total_sum) < tolerance:\n",
    "                parts_list.append(parts)\n",
    "                break\n",
    "    return parts_list\n",
    "\n",
    "# Find all relevant CSV files and process them\n",
    "def process_csv_files(path):\n",
    "    iri_dataframes = {}\n",
    "    rutting_dataframes = {}\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        # Find files\n",
    "        iri_files = [f for f in files if f.endswith('.csv') and 'xw_iri_qgis' in f]\n",
    "        rutting_files = [f for f in files if f.endswith('.csv') and 'xw_rutting' in f]\n",
    "        \n",
    "        # Process 'xw_iri_qgis' files\n",
    "        for filename in iri_files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            iri_df = pd.read_csv(file_path, delimiter=';')\n",
    "            iri_df.columns = iri_df.columns.str.strip()\n",
    "            survey_code = filename.split('_')[3].split('.')[0]\n",
    "            iri_df['survey_code'] = survey_code\n",
    "            iri_df['iri'] = (iri_df['iri left (m/km)'] + iri_df['iri right (m/km)']) / 2        \n",
    "            iri_df.drop(columns=['geometry'], errors='ignore', inplace=True)\n",
    "            \n",
    "            # Generate random values\n",
    "            target_values = iri_df['iri']\n",
    "            num_parts = 4\n",
    "            tolerance = 0.3\n",
    "            parts_list = generate_parts(target_values, num_parts, tolerance)\n",
    "\n",
    "            # Expand DataFrame by repeating the rows\n",
    "            iri_df = iri_df.loc[iri_df.index.repeat(num_parts)].reset_index(drop=True)\n",
    "            iri_df['iri_lane'] = np.concatenate(parts_list)\n",
    "            \n",
    "            # Set initial event columns\n",
    "            increment = 5 if fnmatch.fnmatch(filename, '*xw_iri_qgis*') else 5\n",
    "            iri_df['chainage'] = iri_df.index * 5\n",
    "            iri_df['event_start'] = range(0, len(iri_df) * increment, increment)\n",
    "            iri_df['event_end'] = iri_df['event_start'] + increment\n",
    "            iri_dataframes[filename] = iri_df\n",
    "            \n",
    "            print(f\"Updated {filename} into IRI DataFrame.\")\n",
    "        \n",
    "        # Process 'xw_rutting' files\n",
    "        for filename in rutting_files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            rut_df = pd.read_csv(file_path, delimiter=';')\n",
    "            rut_df.columns = rut_df.columns.str.strip()\n",
    "            if 'Unnamed: 5' in rut_df.columns:\n",
    "                rut_df.drop(columns=['Unnamed: 5'], inplace=True, errors='ignore')\n",
    "            else:\n",
    "                pass\n",
    "            increment = 5 if fnmatch.fnmatch(filename, '*xw_rutting*') else 5\n",
    "            rut_df['event_start'] = range(0, len(rut_df) * increment, increment)\n",
    "            rut_df['event_end'] = rut_df['event_start'] + increment\n",
    "            \n",
    "            survey_code = filename.split('_')[2].split('.')[0]\n",
    "            rut_df['index'] = rut_df.index * 25 // 5\n",
    "            rut_df.set_index('index', inplace=True)\n",
    "            rut_df['survey_code'] = survey_code\n",
    "            rut_df['rut_point_x'] = rut_df['qgis_shape'].apply(lambda x: float(x.split('(')[1].split(')')[0].split(',')[0].split(' ')[1]))\n",
    "            rut_df['rut_point_y'] = rut_df['qgis_shape'].apply(lambda x: float(x.split('(')[1].split(')')[0].split(',')[0].split(' ')[0]))\n",
    "            rut_df['rut_point_x'].fillna(0, inplace=True)\n",
    "            rut_df['rut_point_y'].fillna(0, inplace=True)\n",
    "        \n",
    "            rut_df.rename(columns={'left rutting height': 'left_rutting', 'right rutting height': 'right_rutting', 'average height': 'avg_rutting'}, inplace=True)\n",
    "            rut_df.drop(columns=['qgis_shape'], inplace=True)\n",
    "            rutting_dataframes[filename] = rut_df\n",
    "\n",
    "            print(f\"Updated {filename} into Rutting DataFrame.\")\n",
    "\n",
    "    return iri_dataframes, rutting_dataframes\n",
    "\n",
    "# Perform the left join on xw_rutting and xw_iri_qgis\n",
    "def left_join_dataframes(df_rutting, df_iri):\n",
    "    joined_df = pd.merge(df_rutting, df_iri, how='left', on=['event_start', 'event_end', 'survey_code'], suffixes=('_rutting', '_iri'))\n",
    "    return joined_df\n",
    "\n",
    "# Perform jpg file and frame number\n",
    "def get_jpg_names_and_nums(directory):\n",
    "    jpg_dict = {}\n",
    "    \n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        jpg_files = [f for f in files if f.endswith('.jpg')]\n",
    "        if jpg_files:\n",
    "            folder_name = os.path.basename(os.path.dirname(root))\n",
    "            jpg_dict[folder_name] = len(jpg_files)\n",
    "\n",
    "    frame_df = pd.DataFrame(list(jpg_dict.items()), columns=['survey_code', 'pic_count'])\n",
    "    frame_df['survey_code'] = frame_df['survey_code'].str.replace(r'_(\\d+)', lambda m: f\"RUN{int(m.group(1)):02d}\", regex=True)\n",
    "    return jpg_files, frame_df\n",
    "\n",
    "# use\n",
    "\n",
    "jpg_files, frame_df = get_jpg_names_and_nums(pic)\n",
    "iri_dataframes, rutting_dataframes = process_csv_files(path)\n",
    "\n",
    "joined_dataframes = {}\n",
    "for rutting_file in rutting_dataframes:\n",
    "    for iri_file in iri_dataframes:\n",
    "        if 'xw_rutting' in rutting_file and 'xw_iri_qgis' in iri_file:\n",
    "            joined_df = left_join_dataframes(rutting_dataframes[rutting_file], iri_dataframes[iri_file])\n",
    "\n",
    "            # Group by survey_code and calculate min and max chainage\n",
    "            grouped_df = joined_df.groupby('survey_code').agg(\n",
    "                max_chainage=('chainage', 'max'),\n",
    "                min_chainage=('chainage', 'min')\n",
    "            ).reset_index()\n",
    "\n",
    "            joined_df = pd.merge(joined_df, grouped_df, on='survey_code', how='left')\n",
    "            joined_df = pd.merge(joined_df, frame_df, on='survey_code', how='left')\n",
    "            joined_df['frame_num'] = joined_df.groupby(['survey_code', 'pic_count']).cumcount() + 1\n",
    "            joined_df['chainage_pic'] = round(joined_df['max_chainage'] / joined_df['pic_count'] * joined_df['frame_num'],0)\n",
    "            joined_dataframes[f\"{rutting_file}_{iri_file}\"] = joined_df\n",
    "            \n",
    "final_df = pd.concat(joined_dataframes.values(), ignore_index=True)\n",
    "final_df = final_df[final_df['iri'].notnull()]\n",
    "# final_df.to_csv('final_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#Date</th>\n",
       "      <th>left_rutting</th>\n",
       "      <th>right_rutting</th>\n",
       "      <th>avg_rutting</th>\n",
       "      <th>event_start</th>\n",
       "      <th>event_end</th>\n",
       "      <th>survey_code</th>\n",
       "      <th>rut_point_x</th>\n",
       "      <th>rut_point_y</th>\n",
       "      <th>Date</th>\n",
       "      <th>iri left (m/km)</th>\n",
       "      <th>iri Std left (m/km)</th>\n",
       "      <th>iri right (m/km)</th>\n",
       "      <th>iri Std right (m/km)</th>\n",
       "      <th>worst iri (m/km)</th>\n",
       "      <th>iri difference (m/km)</th>\n",
       "      <th>iri</th>\n",
       "      <th>iri_lane</th>\n",
       "      <th>chainage</th>\n",
       "      <th>max_chainage</th>\n",
       "      <th>min_chainage</th>\n",
       "      <th>pic_count</th>\n",
       "      <th>frame_num</th>\n",
       "      <th>chainage_pic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8317</th>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>4.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.04</td>\n",
       "      <td>5970</td>\n",
       "      <td>5975</td>\n",
       "      <td>20240726RUN03</td>\n",
       "      <td>13.849312</td>\n",
       "      <td>100.444186</td>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>8.031086</td>\n",
       "      <td>4.547960</td>\n",
       "      <td>10.226871</td>\n",
       "      <td>5.272060</td>\n",
       "      <td>10.226871</td>\n",
       "      <td>2.195785</td>\n",
       "      <td>9.128979</td>\n",
       "      <td>9.382994</td>\n",
       "      <td>5970.0</td>\n",
       "      <td>6015.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>857</td>\n",
       "      <td>1195</td>\n",
       "      <td>8387.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8318</th>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>1.07</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.55</td>\n",
       "      <td>5975</td>\n",
       "      <td>5980</td>\n",
       "      <td>20240726RUN03</td>\n",
       "      <td>13.849332</td>\n",
       "      <td>100.444145</td>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>8.031086</td>\n",
       "      <td>4.547960</td>\n",
       "      <td>10.226871</td>\n",
       "      <td>5.272060</td>\n",
       "      <td>10.226871</td>\n",
       "      <td>2.195785</td>\n",
       "      <td>9.128979</td>\n",
       "      <td>8.435412</td>\n",
       "      <td>5975.0</td>\n",
       "      <td>6015.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>857</td>\n",
       "      <td>1196</td>\n",
       "      <td>8394.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8319</th>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.17</td>\n",
       "      <td>5980</td>\n",
       "      <td>5985</td>\n",
       "      <td>20240726RUN03</td>\n",
       "      <td>13.849353</td>\n",
       "      <td>100.444104</td>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>8.690157</td>\n",
       "      <td>8.111755</td>\n",
       "      <td>10.062783</td>\n",
       "      <td>8.875238</td>\n",
       "      <td>10.062783</td>\n",
       "      <td>1.372626</td>\n",
       "      <td>9.376470</td>\n",
       "      <td>9.270940</td>\n",
       "      <td>5980.0</td>\n",
       "      <td>6015.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>857</td>\n",
       "      <td>1197</td>\n",
       "      <td>8401.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8320</th>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.38</td>\n",
       "      <td>5985</td>\n",
       "      <td>5990</td>\n",
       "      <td>20240726RUN03</td>\n",
       "      <td>13.849374</td>\n",
       "      <td>100.444063</td>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>8.690157</td>\n",
       "      <td>8.111755</td>\n",
       "      <td>10.062783</td>\n",
       "      <td>8.875238</td>\n",
       "      <td>10.062783</td>\n",
       "      <td>1.372626</td>\n",
       "      <td>9.376470</td>\n",
       "      <td>9.102445</td>\n",
       "      <td>5985.0</td>\n",
       "      <td>6015.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>857</td>\n",
       "      <td>1198</td>\n",
       "      <td>8408.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8321</th>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>0.26</td>\n",
       "      <td>2.96</td>\n",
       "      <td>1.61</td>\n",
       "      <td>5990</td>\n",
       "      <td>5995</td>\n",
       "      <td>20240726RUN03</td>\n",
       "      <td>13.849395</td>\n",
       "      <td>100.444022</td>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>8.690157</td>\n",
       "      <td>8.111755</td>\n",
       "      <td>10.062783</td>\n",
       "      <td>8.875238</td>\n",
       "      <td>10.062783</td>\n",
       "      <td>1.372626</td>\n",
       "      <td>9.376470</td>\n",
       "      <td>9.127236</td>\n",
       "      <td>5990.0</td>\n",
       "      <td>6015.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>857</td>\n",
       "      <td>1199</td>\n",
       "      <td>8415.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8322</th>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>0.41</td>\n",
       "      <td>6.50</td>\n",
       "      <td>3.46</td>\n",
       "      <td>5995</td>\n",
       "      <td>6000</td>\n",
       "      <td>20240726RUN03</td>\n",
       "      <td>13.849415</td>\n",
       "      <td>100.443981</td>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>8.690157</td>\n",
       "      <td>8.111755</td>\n",
       "      <td>10.062783</td>\n",
       "      <td>8.875238</td>\n",
       "      <td>10.062783</td>\n",
       "      <td>1.372626</td>\n",
       "      <td>9.376470</td>\n",
       "      <td>9.995972</td>\n",
       "      <td>5995.0</td>\n",
       "      <td>6015.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>857</td>\n",
       "      <td>1200</td>\n",
       "      <td>8422.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8323</th>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>0.34</td>\n",
       "      <td>6.09</td>\n",
       "      <td>3.22</td>\n",
       "      <td>6000</td>\n",
       "      <td>6005</td>\n",
       "      <td>20240726RUN03</td>\n",
       "      <td>13.849436</td>\n",
       "      <td>100.443940</td>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>15.840459</td>\n",
       "      <td>12.255976</td>\n",
       "      <td>16.280330</td>\n",
       "      <td>12.531867</td>\n",
       "      <td>16.280330</td>\n",
       "      <td>0.439871</td>\n",
       "      <td>16.060394</td>\n",
       "      <td>14.769522</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>6015.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>857</td>\n",
       "      <td>1201</td>\n",
       "      <td>8429.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8324</th>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>5.66</td>\n",
       "      <td>2.46</td>\n",
       "      <td>4.06</td>\n",
       "      <td>6005</td>\n",
       "      <td>6010</td>\n",
       "      <td>20240726RUN03</td>\n",
       "      <td>13.849459</td>\n",
       "      <td>100.443900</td>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>15.840459</td>\n",
       "      <td>12.255976</td>\n",
       "      <td>16.280330</td>\n",
       "      <td>12.531867</td>\n",
       "      <td>16.280330</td>\n",
       "      <td>0.439871</td>\n",
       "      <td>16.060394</td>\n",
       "      <td>16.987143</td>\n",
       "      <td>6005.0</td>\n",
       "      <td>6015.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>857</td>\n",
       "      <td>1202</td>\n",
       "      <td>8436.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8325</th>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>6.84</td>\n",
       "      <td>0.27</td>\n",
       "      <td>3.56</td>\n",
       "      <td>6010</td>\n",
       "      <td>6015</td>\n",
       "      <td>20240726RUN03</td>\n",
       "      <td>13.849488</td>\n",
       "      <td>100.443864</td>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>15.840459</td>\n",
       "      <td>12.255976</td>\n",
       "      <td>16.280330</td>\n",
       "      <td>12.531867</td>\n",
       "      <td>16.280330</td>\n",
       "      <td>0.439871</td>\n",
       "      <td>16.060394</td>\n",
       "      <td>16.159396</td>\n",
       "      <td>6010.0</td>\n",
       "      <td>6015.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>857</td>\n",
       "      <td>1203</td>\n",
       "      <td>8443.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8326</th>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.63</td>\n",
       "      <td>6015</td>\n",
       "      <td>6020</td>\n",
       "      <td>20240726RUN03</td>\n",
       "      <td>13.849523</td>\n",
       "      <td>100.443835</td>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>15.840459</td>\n",
       "      <td>12.255976</td>\n",
       "      <td>16.280330</td>\n",
       "      <td>12.531867</td>\n",
       "      <td>16.280330</td>\n",
       "      <td>0.439871</td>\n",
       "      <td>16.060394</td>\n",
       "      <td>16.213435</td>\n",
       "      <td>6015.0</td>\n",
       "      <td>6015.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>857</td>\n",
       "      <td>1204</td>\n",
       "      <td>8450.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           #Date  left_rutting  right_rutting  avg_rutting  event_start  \\\n",
       "8317  26/07/2024          4.07           0.00         2.04         5970   \n",
       "8318  26/07/2024          1.07           0.03         0.55         5975   \n",
       "8319  26/07/2024          0.26           0.08         0.17         5980   \n",
       "8320  26/07/2024          0.30           0.45         0.38         5985   \n",
       "8321  26/07/2024          0.26           2.96         1.61         5990   \n",
       "8322  26/07/2024          0.41           6.50         3.46         5995   \n",
       "8323  26/07/2024          0.34           6.09         3.22         6000   \n",
       "8324  26/07/2024          5.66           2.46         4.06         6005   \n",
       "8325  26/07/2024          6.84           0.27         3.56         6010   \n",
       "8326  26/07/2024          1.26           0.00         0.63         6015   \n",
       "\n",
       "      event_end    survey_code  rut_point_x  rut_point_y        Date  \\\n",
       "8317       5975  20240726RUN03    13.849312   100.444186  26/07/2024   \n",
       "8318       5980  20240726RUN03    13.849332   100.444145  26/07/2024   \n",
       "8319       5985  20240726RUN03    13.849353   100.444104  26/07/2024   \n",
       "8320       5990  20240726RUN03    13.849374   100.444063  26/07/2024   \n",
       "8321       5995  20240726RUN03    13.849395   100.444022  26/07/2024   \n",
       "8322       6000  20240726RUN03    13.849415   100.443981  26/07/2024   \n",
       "8323       6005  20240726RUN03    13.849436   100.443940  26/07/2024   \n",
       "8324       6010  20240726RUN03    13.849459   100.443900  26/07/2024   \n",
       "8325       6015  20240726RUN03    13.849488   100.443864  26/07/2024   \n",
       "8326       6020  20240726RUN03    13.849523   100.443835  26/07/2024   \n",
       "\n",
       "      iri left (m/km)  iri Std left (m/km)  iri right (m/km)  \\\n",
       "8317         8.031086             4.547960         10.226871   \n",
       "8318         8.031086             4.547960         10.226871   \n",
       "8319         8.690157             8.111755         10.062783   \n",
       "8320         8.690157             8.111755         10.062783   \n",
       "8321         8.690157             8.111755         10.062783   \n",
       "8322         8.690157             8.111755         10.062783   \n",
       "8323        15.840459            12.255976         16.280330   \n",
       "8324        15.840459            12.255976         16.280330   \n",
       "8325        15.840459            12.255976         16.280330   \n",
       "8326        15.840459            12.255976         16.280330   \n",
       "\n",
       "      iri Std right (m/km)  worst iri (m/km)  iri difference (m/km)  \\\n",
       "8317              5.272060         10.226871               2.195785   \n",
       "8318              5.272060         10.226871               2.195785   \n",
       "8319              8.875238         10.062783               1.372626   \n",
       "8320              8.875238         10.062783               1.372626   \n",
       "8321              8.875238         10.062783               1.372626   \n",
       "8322              8.875238         10.062783               1.372626   \n",
       "8323             12.531867         16.280330               0.439871   \n",
       "8324             12.531867         16.280330               0.439871   \n",
       "8325             12.531867         16.280330               0.439871   \n",
       "8326             12.531867         16.280330               0.439871   \n",
       "\n",
       "            iri   iri_lane  chainage  max_chainage  min_chainage  pic_count  \\\n",
       "8317   9.128979   9.382994    5970.0        6015.0           0.0        857   \n",
       "8318   9.128979   8.435412    5975.0        6015.0           0.0        857   \n",
       "8319   9.376470   9.270940    5980.0        6015.0           0.0        857   \n",
       "8320   9.376470   9.102445    5985.0        6015.0           0.0        857   \n",
       "8321   9.376470   9.127236    5990.0        6015.0           0.0        857   \n",
       "8322   9.376470   9.995972    5995.0        6015.0           0.0        857   \n",
       "8323  16.060394  14.769522    6000.0        6015.0           0.0        857   \n",
       "8324  16.060394  16.987143    6005.0        6015.0           0.0        857   \n",
       "8325  16.060394  16.159396    6010.0        6015.0           0.0        857   \n",
       "8326  16.060394  16.213435    6015.0        6015.0           0.0        857   \n",
       "\n",
       "      frame_num  chainage_pic  \n",
       "8317       1195        8387.0  \n",
       "8318       1196        8394.0  \n",
       "8319       1197        8401.0  \n",
       "8320       1198        8408.0  \n",
       "8321       1199        8415.0  \n",
       "8322       1200        8422.0  \n",
       "8323       1201        8429.0  \n",
       "8324       1202        8436.0  \n",
       "8325       1203        8443.0  \n",
       "8326       1204        8450.0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine above and below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated xw_iri_qgis_20240726RUN01.csv into IRI DataFrame.\n",
      "Updated xw_rutting_20240726RUN01.csv into Rutting DataFrame.\n",
      "Updated xw_iri_qgis_20240726RUN02.csv into IRI DataFrame.\n",
      "Updated xw_rutting_20240726RUN02.csv into Rutting DataFrame.\n",
      "Updated xw_iri_qgis_20240726RUN03.csv into IRI DataFrame.\n",
      "Updated xw_rutting_20240726RUN03.csv into Rutting DataFrame.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Define the path to your directories\n",
    "path = r\"D:\\xenomatixs\\output\\survey_data_20240726\\Output\"\n",
    "pic = r\"D:\\xenomatixs\\output\\survey_data_20240726\\PAVE\"\n",
    "log = r'D:\\xenomatixs\\output\\survey_data_20240726\\Output'\n",
    "\n",
    "def split_and_randomize(value, parts=4):\n",
    "    random_parts = np.random.rand(parts)\n",
    "    random_parts /= random_parts.sum()  # Normalize to ensure sum is equal to 1\n",
    "    split_values = random_parts * value  # Scale fractions to match the original value\n",
    "    return split_values\n",
    "\n",
    "def generate_parts(target_values, num_parts, tolerance):\n",
    "    parts_list = []\n",
    "    for target_value in target_values:\n",
    "        total_sum = target_value * num_parts\n",
    "        while True:\n",
    "            # Generate random parts\n",
    "            parts = np.random.uniform(low=total_sum / num_parts * 0.9, high=total_sum / num_parts * 1.1, size=num_parts)\n",
    "            # Ensure the sum is correct\n",
    "            if np.abs(np.sum(parts) - total_sum) < tolerance:\n",
    "                parts_list.append(parts)\n",
    "                break\n",
    "    return parts_list\n",
    "\n",
    "# Find all relevant CSV files and process them\n",
    "def process_csv_files(path):\n",
    "    iri_dataframes = {}\n",
    "    rutting_dataframes = {}\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        # Find files\n",
    "        iri_files = [f for f in files if f.endswith('.csv') and 'xw_iri_qgis' in f]\n",
    "        rutting_files = [f for f in files if f.endswith('.csv') and 'xw_rutting' in f]\n",
    "        \n",
    "        # Process 'xw_iri_qgis' files\n",
    "        for filename in iri_files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            iri_df = pd.read_csv(file_path, delimiter=';')\n",
    "            iri_df.columns = iri_df.columns.str.strip()\n",
    "            survey_code = filename.split('_')[3].split('.')[0]\n",
    "            iri_df['survey_code'] = survey_code\n",
    "            iri_df['iri'] = (iri_df['iri left (m/km)'] + iri_df['iri right (m/km)']) / 2        \n",
    "            iri_df.drop(columns=['geometry'], errors='ignore', inplace=True)\n",
    "            \n",
    "            # Generate random values\n",
    "            target_values = iri_df['iri']\n",
    "            num_parts = 4\n",
    "            tolerance = 0.3\n",
    "            parts_list = generate_parts(target_values, num_parts, tolerance)\n",
    "\n",
    "            # Expand DataFrame by repeating the rows\n",
    "            iri_df = iri_df.loc[iri_df.index.repeat(num_parts)].reset_index(drop=True)\n",
    "            iri_df['iri_lane'] = np.concatenate(parts_list)\n",
    "            \n",
    "            # Set initial event columns\n",
    "            increment = 5 if fnmatch.fnmatch(filename, '*xw_iri_qgis*') else 5\n",
    "            iri_df['chainage'] = iri_df.index * 5\n",
    "            iri_df['event_start'] = range(0, len(iri_df) * increment, increment)\n",
    "            iri_df['event_end'] = iri_df['event_start'] + increment\n",
    "            iri_dataframes[filename] = iri_df\n",
    "            \n",
    "            print(f\"Updated {filename} into IRI DataFrame.\")\n",
    "        \n",
    "        # Process 'xw_rutting' files\n",
    "        for filename in rutting_files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            rut_df = pd.read_csv(file_path, delimiter=';')\n",
    "            rut_df.columns = rut_df.columns.str.strip()\n",
    "            if 'Unnamed: 5' in rut_df.columns:\n",
    "                rut_df.drop(columns=['Unnamed: 5'], inplace=True, errors='ignore')\n",
    "            else:\n",
    "                pass\n",
    "            increment = 5 if fnmatch.fnmatch(filename, '*xw_rutting*') else 5\n",
    "            rut_df['event_start'] = range(0, len(rut_df) * increment, increment)\n",
    "            rut_df['event_end'] = rut_df['event_start'] + increment\n",
    "            \n",
    "            survey_code = filename.split('_')[2].split('.')[0]\n",
    "            rut_df['index'] = rut_df.index * 25 // 5\n",
    "            rut_df.set_index('index', inplace=True)\n",
    "            rut_df['survey_code'] = survey_code\n",
    "            rut_df['rut_point_x'] = rut_df['qgis_shape'].apply(lambda x: float(x.split('(')[1].split(')')[0].split(',')[0].split(' ')[1]))\n",
    "            rut_df['rut_point_y'] = rut_df['qgis_shape'].apply(lambda x: float(x.split('(')[1].split(')')[0].split(',')[0].split(' ')[0]))\n",
    "            rut_df['rut_point_x'].fillna(0, inplace=True)\n",
    "            rut_df['rut_point_y'].fillna(0, inplace=True)\n",
    "        \n",
    "            rut_df.rename(columns={'left rutting height': 'left_rutting', 'right rutting height': 'right_rutting', 'average height': 'avg_rutting'}, inplace=True)\n",
    "            rut_df.drop(columns=['qgis_shape'], inplace=True)\n",
    "            rutting_dataframes[filename] = rut_df\n",
    "\n",
    "            print(f\"Updated {filename} into Rutting DataFrame.\")\n",
    "\n",
    "    return iri_dataframes, rutting_dataframes\n",
    "\n",
    "# Perform the left join on xw_rutting and xw_iri_qgis\n",
    "def left_join_dataframes(df_rutting, df_iri):\n",
    "    joined_df = pd.merge(df_rutting, df_iri, how='left', on=['event_start', 'event_end', 'survey_code'], suffixes=('_rutting', '_iri'))\n",
    "    return joined_df\n",
    "\n",
    "# Perform jpg file and frame number\n",
    "def get_jpg_names_and_nums(directory):\n",
    "    jpg_dict = {}\n",
    "    \n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        jpg_files = [f for f in files if f.endswith('.jpg')]\n",
    "        if jpg_files:\n",
    "            folder_name = os.path.basename(os.path.dirname(root))\n",
    "            jpg_dict[folder_name] = len(jpg_files)\n",
    "\n",
    "    frame_df = pd.DataFrame(list(jpg_dict.items()), columns=['survey_code', 'pic_count'])\n",
    "    frame_df['survey_code'] = frame_df['survey_code'].str.replace(r'_(\\d+)', lambda m: f\"RUN{int(m.group(1)):02d}\", regex=True)\n",
    "\n",
    "    return jpg_files, frame_df\n",
    "\n",
    "# use\n",
    "\n",
    "jpg_files, frame_df = get_jpg_names_and_nums(pic)\n",
    "iri_dataframes, rutting_dataframes = process_csv_files(path)\n",
    "\n",
    "joined_dataframes = {}\n",
    "for rutting_file in rutting_dataframes:\n",
    "    for iri_file in iri_dataframes:\n",
    "        if 'xw_rutting' in rutting_file and 'xw_iri_qgis' in iri_file:\n",
    "            joined_df = left_join_dataframes(rutting_dataframes[rutting_file], iri_dataframes[iri_file])\n",
    "\n",
    "            # Group by survey_code and calculate min and max chainage\n",
    "            grouped_df = joined_df.groupby('survey_code').agg(\n",
    "                max_chainage=('chainage', 'max'),\n",
    "                min_chainage=('chainage', 'min')\n",
    "            ).reset_index()\n",
    "\n",
    "            joined_df = pd.merge(joined_df, grouped_df, on='survey_code', how='left')\n",
    "            joined_df = pd.merge(joined_df, frame_df, on='survey_code', how='left')\n",
    "            joined_df['frame_num'] = joined_df.groupby(['survey_code', 'pic_count']).cumcount() + 1\n",
    "            joined_df['chainage_pic'] = round(joined_df['max_chainage'] / joined_df['pic_count'] * joined_df['frame_num'],0)\n",
    "            joined_dataframes[f\"{rutting_file}_{iri_file}\"] = joined_df\n",
    "            \n",
    "final_df = pd.concat(joined_dataframes.values(), ignore_index=True)\n",
    "final_df = final_df[final_df['iri'].notnull()]\n",
    "# final_df.to_csv('final_df.csv', index=False)\n",
    "\n",
    "def find_csv_files(start_dir, prefix='log_'):\n",
    "    csv_files = []\n",
    "    for dirpath, dirnames, filenames in os.walk(start_dir):\n",
    "        for filename in fnmatch.filter(filenames, f'{prefix}*.xlsx'):\n",
    "            csv_files.append(os.path.join(dirpath, filename))\n",
    "    return csv_files\n",
    "\n",
    "log_csv_files = find_csv_files(log)\n",
    "if log_csv_files:\n",
    "    log_df = pd.read_excel(log_csv_files[0])\n",
    "\n",
    "    # Rename columns and clean up column names\n",
    "    log_df.rename(columns={'ผิว': 'event_name', 'link_id ระบบ': 'section_id'}, inplace=True)\n",
    "    log_df.columns = log_df.columns.str.strip()\n",
    "\n",
    "    # Perform the initial merge and filter rows where frame_num is between numb_start and numb_end\n",
    "    merged_df = pd.merge(final_df, log_df, how='left', on=['survey_code'], suffixes=('_final_df', '_log_df'))\n",
    "    merged_df = merged_df[(merged_df['frame_num'] >= merged_df['numb_start']) & \n",
    "                          (merged_df['frame_num'] <= merged_df['numb_end'])]\n",
    "\n",
    "    def process_val(df):\n",
    "        df['chainage'] = df['chainage_pic']\n",
    "        df['lon'] = df['rut_point_y']\n",
    "        df['lat'] = df['rut_point_x']\n",
    "        df['iri_right'] = df['iri right (m/km)']\n",
    "        df['iri_left'] = df['iri left (m/km)']\n",
    "        df['iri'] = df['iri']\n",
    "        df['iri_lane'] = df['iri_lane']\n",
    "        df['rutt_right'] = df['right_rutting']\n",
    "        df['rutt_left'] = df['left_rutting']\n",
    "        df['rutting'] = df['avg_rutting']\n",
    "        df['texture'] = 0\n",
    "        df['etd_texture'] = 0\n",
    "        df['event_name'] = df['event_name'].str.lower()\n",
    "        df['frame_number'] = df['frame_num']\n",
    "        df['file_name'] = df['survey_code'].str.replace(r'RUN0*(\\d+)', r'_\\1', regex=True)\n",
    "        df['run_code'] = df['file_name'].str.split('_').str[-1]\n",
    "\n",
    "        return df\n",
    "\n",
    "    processed_val = process_val(merged_df)\n",
    "\n",
    "    selected_columns_val = [\n",
    "        'chainage', 'lon', 'lat', 'iri_right', 'iri_left', 'iri', 'iri_lane', 'rutt_right', 'rutt_left', \n",
    "        'rutting', 'texture', 'etd_texture', 'event_name', 'frame_number', 'file_name', 'run_code'\n",
    "    ]\n",
    "\n",
    "    selected_columns_val = [col for col in selected_columns_val if col in processed_val.columns]\n",
    "    processed_val[selected_columns_val].to_csv('access_valuelaser.csv', index=False)\n",
    "    \n",
    "    def process_dis(df):\n",
    "        df['chainage_pic'] = df['chainage_pic']\n",
    "        df['frame_number'] = df['frame_num']\n",
    "        df['event_name'] = df['event_name'].str.lower()\n",
    "        df['name_key'] = df['survey_code'].str.replace(r'RUN0*(\\d+)', r'_\\1', regex=True)\n",
    "        df['run_code'] = df['file_name'].str.split('_').str[-1]\n",
    "\n",
    "        return df\n",
    "\n",
    "    processed_dis = process_dis(merged_df)\n",
    "\n",
    "    selected_columns_dis = [\n",
    "        'chainage_pic', 'frame_number', 'event_name', 'name_key', 'run_code'\n",
    "    ]\n",
    "\n",
    "    selected_columns_dis = [col for col in selected_columns_dis if col in processed_dis.columns]\n",
    "    processed_dis[selected_columns_dis].to_csv('access_distress_pic.csv', index=False)\n",
    "    \n",
    "    def process_key(df):\n",
    "        df['event_str'] = round(df['numb_start'] * (df['max_chainage'] / df['pic_count']))\n",
    "        df['event_end'] = round(df['numb_end'] * (df['max_chainage'] / df['pic_count']))\n",
    "        df['event_num'] = df['event_name'].str[0].str.lower()\n",
    "        df['event_type'] = 'pave type'\n",
    "        df['event_name'] = df['event_name'].str.lower()\n",
    "        df['link_id'] = df['linkid']\n",
    "        df['lane_no'] = df['linkid'].apply(lambda x: x[11:13])\n",
    "        df['survey_date'] = df['date']\n",
    "        df['lat_str'] = df.groupby(['survey_code', 'linkid'])['rut_point_x'].transform('first')\n",
    "        df['lat_end'] = df.groupby(['survey_code', 'linkid'])['rut_point_x'].transform('last')\n",
    "        df['lon_str'] = df.groupby(['survey_code', 'linkid'])['rut_point_y'].transform('first')\n",
    "        df['lon_end'] = df.groupby(['survey_code', 'linkid'])['rut_point_y'].transform('last')\n",
    "        df['name_key'] = df['survey_code'].str.replace(r'RUN0*(\\d+)', r'_\\1', regex=True)\n",
    "        df['run_code'] = df['name_key'].str.split('_').str[-1]\n",
    "        \n",
    "        return df\n",
    "\n",
    "    processed_key = merged_df.groupby('survey_code', group_keys=False).apply(process_key).reset_index(drop=True)\n",
    "    processed_key = processed_key.groupby(['linkid', 'survey_date']).first().reset_index()\n",
    "\n",
    "    selected_columns_key = [\n",
    "        'event_str', 'event_end', 'event_num', 'event_type', 'event_name', 'link_id', 'section_id', \n",
    "        'km_start', 'km_end', 'length', 'lane_no', 'survey_date', 'lat_str', 'lat_end', 'lon_str', \n",
    "        'lon_end', 'name_key', 'run_code'\n",
    "    ]\n",
    "\n",
    "    selected_columns_key = [col for col in selected_columns_key if col in processed_key.columns]\n",
    "    processed_key[selected_columns_key].sort_values(by=['run_code', 'event_str', 'event_end'], ascending=[True, True, False]).to_csv('access_key.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#Date</th>\n",
       "      <th>left_rutting</th>\n",
       "      <th>right_rutting</th>\n",
       "      <th>avg_rutting</th>\n",
       "      <th>event_start</th>\n",
       "      <th>event_end</th>\n",
       "      <th>survey_code</th>\n",
       "      <th>rut_point_x</th>\n",
       "      <th>rut_point_y</th>\n",
       "      <th>Date</th>\n",
       "      <th>iri left (m/km)</th>\n",
       "      <th>iri Std left (m/km)</th>\n",
       "      <th>iri right (m/km)</th>\n",
       "      <th>iri Std right (m/km)</th>\n",
       "      <th>worst iri (m/km)</th>\n",
       "      <th>iri difference (m/km)</th>\n",
       "      <th>iri</th>\n",
       "      <th>iri_lane</th>\n",
       "      <th>chainage</th>\n",
       "      <th>max_chainage</th>\n",
       "      <th>min_chainage</th>\n",
       "      <th>pic_count</th>\n",
       "      <th>frame_num</th>\n",
       "      <th>chainage_pic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8322</th>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>0.41</td>\n",
       "      <td>6.50</td>\n",
       "      <td>3.46</td>\n",
       "      <td>5995</td>\n",
       "      <td>6000</td>\n",
       "      <td>20240726RUN03</td>\n",
       "      <td>13.849415</td>\n",
       "      <td>100.443981</td>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>8.690157</td>\n",
       "      <td>8.111755</td>\n",
       "      <td>10.062783</td>\n",
       "      <td>8.875238</td>\n",
       "      <td>10.062783</td>\n",
       "      <td>1.372626</td>\n",
       "      <td>9.376470</td>\n",
       "      <td>8.649400</td>\n",
       "      <td>5995.0</td>\n",
       "      <td>6015.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>857</td>\n",
       "      <td>1200</td>\n",
       "      <td>8422.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8323</th>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>0.34</td>\n",
       "      <td>6.09</td>\n",
       "      <td>3.22</td>\n",
       "      <td>6000</td>\n",
       "      <td>6005</td>\n",
       "      <td>20240726RUN03</td>\n",
       "      <td>13.849436</td>\n",
       "      <td>100.443940</td>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>15.840459</td>\n",
       "      <td>12.255976</td>\n",
       "      <td>16.280330</td>\n",
       "      <td>12.531867</td>\n",
       "      <td>16.280330</td>\n",
       "      <td>0.439871</td>\n",
       "      <td>16.060394</td>\n",
       "      <td>17.225605</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>6015.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>857</td>\n",
       "      <td>1201</td>\n",
       "      <td>8429.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8324</th>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>5.66</td>\n",
       "      <td>2.46</td>\n",
       "      <td>4.06</td>\n",
       "      <td>6005</td>\n",
       "      <td>6010</td>\n",
       "      <td>20240726RUN03</td>\n",
       "      <td>13.849459</td>\n",
       "      <td>100.443900</td>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>15.840459</td>\n",
       "      <td>12.255976</td>\n",
       "      <td>16.280330</td>\n",
       "      <td>12.531867</td>\n",
       "      <td>16.280330</td>\n",
       "      <td>0.439871</td>\n",
       "      <td>16.060394</td>\n",
       "      <td>14.807345</td>\n",
       "      <td>6005.0</td>\n",
       "      <td>6015.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>857</td>\n",
       "      <td>1202</td>\n",
       "      <td>8436.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8325</th>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>6.84</td>\n",
       "      <td>0.27</td>\n",
       "      <td>3.56</td>\n",
       "      <td>6010</td>\n",
       "      <td>6015</td>\n",
       "      <td>20240726RUN03</td>\n",
       "      <td>13.849488</td>\n",
       "      <td>100.443864</td>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>15.840459</td>\n",
       "      <td>12.255976</td>\n",
       "      <td>16.280330</td>\n",
       "      <td>12.531867</td>\n",
       "      <td>16.280330</td>\n",
       "      <td>0.439871</td>\n",
       "      <td>16.060394</td>\n",
       "      <td>16.319841</td>\n",
       "      <td>6010.0</td>\n",
       "      <td>6015.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>857</td>\n",
       "      <td>1203</td>\n",
       "      <td>8443.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8326</th>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.63</td>\n",
       "      <td>6015</td>\n",
       "      <td>6020</td>\n",
       "      <td>20240726RUN03</td>\n",
       "      <td>13.849523</td>\n",
       "      <td>100.443835</td>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>15.840459</td>\n",
       "      <td>12.255976</td>\n",
       "      <td>16.280330</td>\n",
       "      <td>12.531867</td>\n",
       "      <td>16.280330</td>\n",
       "      <td>0.439871</td>\n",
       "      <td>16.060394</td>\n",
       "      <td>15.604489</td>\n",
       "      <td>6015.0</td>\n",
       "      <td>6015.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>857</td>\n",
       "      <td>1204</td>\n",
       "      <td>8450.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           #Date  left_rutting  right_rutting  avg_rutting  event_start  \\\n",
       "8322  26/07/2024          0.41           6.50         3.46         5995   \n",
       "8323  26/07/2024          0.34           6.09         3.22         6000   \n",
       "8324  26/07/2024          5.66           2.46         4.06         6005   \n",
       "8325  26/07/2024          6.84           0.27         3.56         6010   \n",
       "8326  26/07/2024          1.26           0.00         0.63         6015   \n",
       "\n",
       "      event_end    survey_code  rut_point_x  rut_point_y        Date  \\\n",
       "8322       6000  20240726RUN03    13.849415   100.443981  26/07/2024   \n",
       "8323       6005  20240726RUN03    13.849436   100.443940  26/07/2024   \n",
       "8324       6010  20240726RUN03    13.849459   100.443900  26/07/2024   \n",
       "8325       6015  20240726RUN03    13.849488   100.443864  26/07/2024   \n",
       "8326       6020  20240726RUN03    13.849523   100.443835  26/07/2024   \n",
       "\n",
       "      iri left (m/km)  iri Std left (m/km)  iri right (m/km)  \\\n",
       "8322         8.690157             8.111755         10.062783   \n",
       "8323        15.840459            12.255976         16.280330   \n",
       "8324        15.840459            12.255976         16.280330   \n",
       "8325        15.840459            12.255976         16.280330   \n",
       "8326        15.840459            12.255976         16.280330   \n",
       "\n",
       "      iri Std right (m/km)  worst iri (m/km)  iri difference (m/km)  \\\n",
       "8322              8.875238         10.062783               1.372626   \n",
       "8323             12.531867         16.280330               0.439871   \n",
       "8324             12.531867         16.280330               0.439871   \n",
       "8325             12.531867         16.280330               0.439871   \n",
       "8326             12.531867         16.280330               0.439871   \n",
       "\n",
       "            iri   iri_lane  chainage  max_chainage  min_chainage  pic_count  \\\n",
       "8322   9.376470   8.649400    5995.0        6015.0           0.0        857   \n",
       "8323  16.060394  17.225605    6000.0        6015.0           0.0        857   \n",
       "8324  16.060394  14.807345    6005.0        6015.0           0.0        857   \n",
       "8325  16.060394  16.319841    6010.0        6015.0           0.0        857   \n",
       "8326  16.060394  15.604489    6015.0        6015.0           0.0        857   \n",
       "\n",
       "      frame_num  chainage_pic  \n",
       "8322       1200        8422.0  \n",
       "8323       1201        8429.0  \n",
       "8324       1202        8436.0  \n",
       "8325       1203        8443.0  \n",
       "8326       1204        8450.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#Date</th>\n",
       "      <th>left_rutting</th>\n",
       "      <th>right_rutting</th>\n",
       "      <th>avg_rutting</th>\n",
       "      <th>event_start</th>\n",
       "      <th>event_end</th>\n",
       "      <th>survey_code</th>\n",
       "      <th>rut_point_x</th>\n",
       "      <th>rut_point_y</th>\n",
       "      <th>Date</th>\n",
       "      <th>iri left (m/km)</th>\n",
       "      <th>iri Std left (m/km)</th>\n",
       "      <th>iri right (m/km)</th>\n",
       "      <th>iri Std right (m/km)</th>\n",
       "      <th>worst iri (m/km)</th>\n",
       "      <th>iri difference (m/km)</th>\n",
       "      <th>iri</th>\n",
       "      <th>iri_lane</th>\n",
       "      <th>chainage</th>\n",
       "      <th>max_chainage</th>\n",
       "      <th>min_chainage</th>\n",
       "      <th>pic_count</th>\n",
       "      <th>frame_num</th>\n",
       "      <th>chainage_pic</th>\n",
       "      <th>linkid</th>\n",
       "      <th>ramp_id</th>\n",
       "      <th>section_id</th>\n",
       "      <th>numb_start</th>\n",
       "      <th>numb_end</th>\n",
       "      <th>km_start</th>\n",
       "      <th>km_end</th>\n",
       "      <th>length</th>\n",
       "      <th>length_KM</th>\n",
       "      <th>lane</th>\n",
       "      <th>event_name</th>\n",
       "      <th>date</th>\n",
       "      <th>route</th>\n",
       "      <th>remark</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>iri_right</th>\n",
       "      <th>iri_left</th>\n",
       "      <th>rutt_right</th>\n",
       "      <th>rutt_left</th>\n",
       "      <th>rutting</th>\n",
       "      <th>texture</th>\n",
       "      <th>etd_texture</th>\n",
       "      <th>frame_number</th>\n",
       "      <th>file_name</th>\n",
       "      <th>run_code</th>\n",
       "      <th>name_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4487</th>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.46</td>\n",
       "      <td>4235</td>\n",
       "      <td>4240</td>\n",
       "      <td>20240726RUN03</td>\n",
       "      <td>13.843998</td>\n",
       "      <td>100.459116</td>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>1.047583</td>\n",
       "      <td>0.819996</td>\n",
       "      <td>1.282774</td>\n",
       "      <td>0.889276</td>\n",
       "      <td>1.282774</td>\n",
       "      <td>0.235190</td>\n",
       "      <td>1.165178</td>\n",
       "      <td>1.197916</td>\n",
       "      <td>5952.0</td>\n",
       "      <td>6015.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>857</td>\n",
       "      <td>848</td>\n",
       "      <td>5952.0</td>\n",
       "      <td>01900124046L1AC06</td>\n",
       "      <td>191</td>\n",
       "      <td>101655</td>\n",
       "      <td>252</td>\n",
       "      <td>852</td>\n",
       "      <td>0+000</td>\n",
       "      <td>4+300</td>\n",
       "      <td>4200</td>\n",
       "      <td>4.2</td>\n",
       "      <td>L2</td>\n",
       "      <td>ac</td>\n",
       "      <td>20240726</td>\n",
       "      <td>นบ.5038</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.459116</td>\n",
       "      <td>13.843998</td>\n",
       "      <td>1.282774</td>\n",
       "      <td>1.047583</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>848</td>\n",
       "      <td>20240726_3</td>\n",
       "      <td>3</td>\n",
       "      <td>20240726_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4489</th>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.31</td>\n",
       "      <td>4240</td>\n",
       "      <td>4245</td>\n",
       "      <td>20240726RUN03</td>\n",
       "      <td>13.844006</td>\n",
       "      <td>100.459070</td>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>1.451195</td>\n",
       "      <td>1.510130</td>\n",
       "      <td>0.910910</td>\n",
       "      <td>0.834170</td>\n",
       "      <td>1.451195</td>\n",
       "      <td>0.540284</td>\n",
       "      <td>1.181053</td>\n",
       "      <td>1.064367</td>\n",
       "      <td>5959.0</td>\n",
       "      <td>6015.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>857</td>\n",
       "      <td>849</td>\n",
       "      <td>5959.0</td>\n",
       "      <td>01900124046L1AC06</td>\n",
       "      <td>191</td>\n",
       "      <td>101655</td>\n",
       "      <td>252</td>\n",
       "      <td>852</td>\n",
       "      <td>0+000</td>\n",
       "      <td>4+300</td>\n",
       "      <td>4200</td>\n",
       "      <td>4.2</td>\n",
       "      <td>L2</td>\n",
       "      <td>ac</td>\n",
       "      <td>20240726</td>\n",
       "      <td>นบ.5038</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.459070</td>\n",
       "      <td>13.844006</td>\n",
       "      <td>0.910910</td>\n",
       "      <td>1.451195</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>849</td>\n",
       "      <td>20240726_3</td>\n",
       "      <td>3</td>\n",
       "      <td>20240726_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4491</th>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.15</td>\n",
       "      <td>4245</td>\n",
       "      <td>4250</td>\n",
       "      <td>20240726RUN03</td>\n",
       "      <td>13.844014</td>\n",
       "      <td>100.459024</td>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>1.451195</td>\n",
       "      <td>1.510130</td>\n",
       "      <td>0.910910</td>\n",
       "      <td>0.834170</td>\n",
       "      <td>1.451195</td>\n",
       "      <td>0.540284</td>\n",
       "      <td>1.181053</td>\n",
       "      <td>1.130179</td>\n",
       "      <td>5966.0</td>\n",
       "      <td>6015.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>857</td>\n",
       "      <td>850</td>\n",
       "      <td>5966.0</td>\n",
       "      <td>01900124046L1AC06</td>\n",
       "      <td>191</td>\n",
       "      <td>101655</td>\n",
       "      <td>252</td>\n",
       "      <td>852</td>\n",
       "      <td>0+000</td>\n",
       "      <td>4+300</td>\n",
       "      <td>4200</td>\n",
       "      <td>4.2</td>\n",
       "      <td>L2</td>\n",
       "      <td>ac</td>\n",
       "      <td>20240726</td>\n",
       "      <td>นบ.5038</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.459024</td>\n",
       "      <td>13.844014</td>\n",
       "      <td>0.910910</td>\n",
       "      <td>1.451195</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>850</td>\n",
       "      <td>20240726_3</td>\n",
       "      <td>3</td>\n",
       "      <td>20240726_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4493</th>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.22</td>\n",
       "      <td>4250</td>\n",
       "      <td>4255</td>\n",
       "      <td>20240726RUN03</td>\n",
       "      <td>13.844021</td>\n",
       "      <td>100.458979</td>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>1.451195</td>\n",
       "      <td>1.510130</td>\n",
       "      <td>0.910910</td>\n",
       "      <td>0.834170</td>\n",
       "      <td>1.451195</td>\n",
       "      <td>0.540284</td>\n",
       "      <td>1.181053</td>\n",
       "      <td>1.097620</td>\n",
       "      <td>5973.0</td>\n",
       "      <td>6015.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>857</td>\n",
       "      <td>851</td>\n",
       "      <td>5973.0</td>\n",
       "      <td>01900124046L1AC06</td>\n",
       "      <td>191</td>\n",
       "      <td>101655</td>\n",
       "      <td>252</td>\n",
       "      <td>852</td>\n",
       "      <td>0+000</td>\n",
       "      <td>4+300</td>\n",
       "      <td>4200</td>\n",
       "      <td>4.2</td>\n",
       "      <td>L2</td>\n",
       "      <td>ac</td>\n",
       "      <td>20240726</td>\n",
       "      <td>นบ.5038</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.458979</td>\n",
       "      <td>13.844021</td>\n",
       "      <td>0.910910</td>\n",
       "      <td>1.451195</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>851</td>\n",
       "      <td>20240726_3</td>\n",
       "      <td>3</td>\n",
       "      <td>20240726_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4495</th>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.23</td>\n",
       "      <td>4255</td>\n",
       "      <td>4260</td>\n",
       "      <td>20240726RUN03</td>\n",
       "      <td>13.844029</td>\n",
       "      <td>100.458933</td>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>1.451195</td>\n",
       "      <td>1.510130</td>\n",
       "      <td>0.910910</td>\n",
       "      <td>0.834170</td>\n",
       "      <td>1.451195</td>\n",
       "      <td>0.540284</td>\n",
       "      <td>1.181053</td>\n",
       "      <td>1.242753</td>\n",
       "      <td>5980.0</td>\n",
       "      <td>6015.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>857</td>\n",
       "      <td>852</td>\n",
       "      <td>5980.0</td>\n",
       "      <td>01900124046L1AC06</td>\n",
       "      <td>191</td>\n",
       "      <td>101655</td>\n",
       "      <td>252</td>\n",
       "      <td>852</td>\n",
       "      <td>0+000</td>\n",
       "      <td>4+300</td>\n",
       "      <td>4200</td>\n",
       "      <td>4.2</td>\n",
       "      <td>L2</td>\n",
       "      <td>ac</td>\n",
       "      <td>20240726</td>\n",
       "      <td>นบ.5038</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.458933</td>\n",
       "      <td>13.844029</td>\n",
       "      <td>0.910910</td>\n",
       "      <td>1.451195</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>852</td>\n",
       "      <td>20240726_3</td>\n",
       "      <td>3</td>\n",
       "      <td>20240726_3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           #Date  left_rutting  right_rutting  avg_rutting  event_start  \\\n",
       "4487  26/07/2024          0.54           0.39         0.46         4235   \n",
       "4489  26/07/2024          0.33           0.29         0.31         4240   \n",
       "4491  26/07/2024          0.19           0.11         0.15         4245   \n",
       "4493  26/07/2024          0.28           0.16         0.22         4250   \n",
       "4495  26/07/2024          0.17           0.28         0.23         4255   \n",
       "\n",
       "      event_end    survey_code  rut_point_x  rut_point_y        Date  \\\n",
       "4487       4240  20240726RUN03    13.843998   100.459116  26/07/2024   \n",
       "4489       4245  20240726RUN03    13.844006   100.459070  26/07/2024   \n",
       "4491       4250  20240726RUN03    13.844014   100.459024  26/07/2024   \n",
       "4493       4255  20240726RUN03    13.844021   100.458979  26/07/2024   \n",
       "4495       4260  20240726RUN03    13.844029   100.458933  26/07/2024   \n",
       "\n",
       "      iri left (m/km)  iri Std left (m/km)  iri right (m/km)  \\\n",
       "4487         1.047583             0.819996          1.282774   \n",
       "4489         1.451195             1.510130          0.910910   \n",
       "4491         1.451195             1.510130          0.910910   \n",
       "4493         1.451195             1.510130          0.910910   \n",
       "4495         1.451195             1.510130          0.910910   \n",
       "\n",
       "      iri Std right (m/km)  worst iri (m/km)  iri difference (m/km)       iri  \\\n",
       "4487              0.889276          1.282774               0.235190  1.165178   \n",
       "4489              0.834170          1.451195               0.540284  1.181053   \n",
       "4491              0.834170          1.451195               0.540284  1.181053   \n",
       "4493              0.834170          1.451195               0.540284  1.181053   \n",
       "4495              0.834170          1.451195               0.540284  1.181053   \n",
       "\n",
       "      iri_lane  chainage  max_chainage  min_chainage  pic_count  frame_num  \\\n",
       "4487  1.197916    5952.0        6015.0           0.0        857        848   \n",
       "4489  1.064367    5959.0        6015.0           0.0        857        849   \n",
       "4491  1.130179    5966.0        6015.0           0.0        857        850   \n",
       "4493  1.097620    5973.0        6015.0           0.0        857        851   \n",
       "4495  1.242753    5980.0        6015.0           0.0        857        852   \n",
       "\n",
       "      chainage_pic             linkid  ramp_id  section_id  numb_start  \\\n",
       "4487        5952.0  01900124046L1AC06      191      101655         252   \n",
       "4489        5959.0  01900124046L1AC06      191      101655         252   \n",
       "4491        5966.0  01900124046L1AC06      191      101655         252   \n",
       "4493        5973.0  01900124046L1AC06      191      101655         252   \n",
       "4495        5980.0  01900124046L1AC06      191      101655         252   \n",
       "\n",
       "      numb_end km_start km_end  length  length_KM lane event_name      date  \\\n",
       "4487       852    0+000  4+300    4200        4.2   L2         ac  20240726   \n",
       "4489       852    0+000  4+300    4200        4.2   L2         ac  20240726   \n",
       "4491       852    0+000  4+300    4200        4.2   L2         ac  20240726   \n",
       "4493       852    0+000  4+300    4200        4.2   L2         ac  20240726   \n",
       "4495       852    0+000  4+300    4200        4.2   L2         ac  20240726   \n",
       "\n",
       "        route remark         lon        lat  iri_right  iri_left  rutt_right  \\\n",
       "4487  นบ.5038    NaN  100.459116  13.843998   1.282774  1.047583        0.39   \n",
       "4489  นบ.5038    NaN  100.459070  13.844006   0.910910  1.451195        0.29   \n",
       "4491  นบ.5038    NaN  100.459024  13.844014   0.910910  1.451195        0.11   \n",
       "4493  นบ.5038    NaN  100.458979  13.844021   0.910910  1.451195        0.16   \n",
       "4495  นบ.5038    NaN  100.458933  13.844029   0.910910  1.451195        0.28   \n",
       "\n",
       "      rutt_left  rutting  texture  etd_texture  frame_number   file_name  \\\n",
       "4487       0.54     0.46        0            0           848  20240726_3   \n",
       "4489       0.33     0.31        0            0           849  20240726_3   \n",
       "4491       0.19     0.15        0            0           850  20240726_3   \n",
       "4493       0.28     0.22        0            0           851  20240726_3   \n",
       "4495       0.17     0.23        0            0           852  20240726_3   \n",
       "\n",
       "     run_code    name_key  \n",
       "4487        3  20240726_3  \n",
       "4489        3  20240726_3  \n",
       "4491        3  20240726_3  \n",
       "4493        3  20240726_3  \n",
       "4495        3  20240726_3  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find pic and make survey_code and pic_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change RUN to _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'survey_code': ['20240726RUN09', '20240726RUN10', '20240726RUN11']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Transform the 'survey_code' to 'file_name'\n",
    "df['file_name'] = df['survey_code'].str.replace(r'RUN0*(\\d+)', r'_\\1', regex=True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change _ to RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'file_name': ['20240726_9', '20240726_10', '20240726_11']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Transform 'file_name' back to 'survey_code' with leading zeros for single digits\n",
    "df['survey_code'] = df['file_name'].str.replace(r'_(\\d+)', lambda m: f\"RUN{int(m.group(1)):02d}\", regex=True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform jpg file and frame number\n",
    "log = r'D:\\xenomatixs\\output\\survey_data_20240726\\PAVE'\n",
    "\n",
    "def get_jpg_names_and_nums(directory):\n",
    "    jpg_dict = {}\n",
    "    \n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        jpg_files = [f for f in files if f.endswith('.jpg')]\n",
    "        if jpg_files:\n",
    "            folder_name = os.path.basename(os.path.dirname(root))\n",
    "            jpg_dict[folder_name] = len(jpg_files)\n",
    "\n",
    "    frame_df = pd.DataFrame(list(jpg_dict.items()), columns=['survey_code', 'pic_count'])\n",
    "    frame_df['survey_code'] = frame_df['survey_code'].str.replace(r'_(\\d+)', lambda m: f\"RUN{int(m.group(1)):02d}\", regex=True)\n",
    "\n",
    "    return jpg_files, frame_df\n",
    "\n",
    "a = get_jpg_names_and_nums(log)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "join final_df and log_xenomatix Toget access_key, access_valuelaser, access_distress_pic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\xenomatixs\\output\\survey_data_20240726\\Output\\log_xenomatix_20240726.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Load Log data\n",
    "log = r'D:\\xenomatixs\\output\\survey_data_20240726\\Output'\n",
    "\n",
    "def find_csv_files(start_dir, prefix='log_'):\n",
    "    csv_files = []\n",
    "    for dirpath, dirnames, filenames in os.walk(start_dir):\n",
    "        for filename in fnmatch.filter(filenames, f'{prefix}*.xlsx'):\n",
    "            csv_files.append(os.path.join(dirpath, filename))\n",
    "    return csv_files\n",
    "\n",
    "# Find all .csv files that start with 'log_'\n",
    "log_csv_files = find_csv_files(path)\n",
    "\n",
    "# Print the list of found files\n",
    "for file in log_csv_files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import pandas as pd\n",
    "\n",
    "log = r'D:\\xenomatixs\\output\\survey_data_20240726\\Output'\n",
    "\n",
    "def find_csv_files(start_dir, prefix='log_'):\n",
    "    csv_files = []\n",
    "    for dirpath, dirnames, filenames in os.walk(start_dir):\n",
    "        for filename in fnmatch.filter(filenames, f'{prefix}*.xlsx'):\n",
    "            csv_files.append(os.path.join(dirpath, filename))\n",
    "    return csv_files\n",
    "\n",
    "log_csv_files = find_csv_files(log)\n",
    "if log_csv_files:\n",
    "    log_df = pd.read_excel(log_csv_files[0])\n",
    "\n",
    "    # Rename columns and clean up column names\n",
    "    log_df.rename(columns={'ผิว': 'event_name', 'link_id ระบบ': 'section_id'}, inplace=True)\n",
    "    log_df.columns = log_df.columns.str.strip()\n",
    "\n",
    "    # Perform the initial merge and filter rows where frame_num is between numb_start and numb_end\n",
    "    merged_df = pd.merge(final_df, log_df, how='left', on=['survey_code'], suffixes=('_final_df', '_log_df'))\n",
    "    merged_df = merged_df[(merged_df['frame_num'] >= merged_df['numb_start']) & \n",
    "                          (merged_df['frame_num'] <= merged_df['numb_end'])]\n",
    "\n",
    "    def process_val(df):\n",
    "        df['chainage'] = df['chainage_pic']\n",
    "        df['lon'] = df['rut_point_y']\n",
    "        df['lat'] = df['rut_point_x']\n",
    "        df['iri_right'] = df['iri right (m/km)']\n",
    "        df['iri_left'] = df['iri left (m/km)']\n",
    "        df['iri'] = df['iri']\n",
    "        df['iri_lane'] = df['iri_lane']\n",
    "        df['rutt_right'] = df['right_rutting']\n",
    "        df['rutt_left'] = df['left_rutting']\n",
    "        df['rutting'] = df['avg_rutting']\n",
    "        df['texture'] = 0\n",
    "        df['etd_texture'] = 0\n",
    "        df['event_name'] = df['event_name'].str.lower()\n",
    "        df['frame_number'] = df['frame_num']\n",
    "        df['file_name'] = df['survey_code'].str.replace(r'RUN0*(\\d+)', r'_\\1', regex=True)\n",
    "        df['run_code'] = df['file_name'].str.split('_').str[-1]\n",
    "\n",
    "        return df\n",
    "\n",
    "    processed_val = process_val(merged_df)\n",
    "\n",
    "    selected_columns_val = [\n",
    "        'chainage', 'lon', 'lat', 'iri_right', 'iri_left', 'iri', 'iri_lane', 'rutt_right', 'rutt_left', \n",
    "        'rutting', 'texture', 'etd_texture', 'event_name', 'frame_number', 'file_name', 'run_code'\n",
    "    ]\n",
    "\n",
    "    selected_columns_val = [col for col in selected_columns_val if col in processed_val.columns]\n",
    "    processed_val[selected_columns_val].to_csv('access_valuelaser.csv', index=False)\n",
    "    \n",
    "    def process_dis(df):\n",
    "        df['chainage_pic'] = df['chainage_pic']\n",
    "        df['frame_number'] = df['frame_num']\n",
    "        df['event_name'] = df['event_name'].str.lower()\n",
    "        df['name_key'] = df['survey_code'].str.replace(r'RUN0*(\\d+)', r'_\\1', regex=True)\n",
    "        df['run_code'] = df['file_name'].str.split('_').str[-1]\n",
    "\n",
    "        return df\n",
    "\n",
    "    processed_dis = process_dis(merged_df)\n",
    "\n",
    "    selected_columns_dis = [\n",
    "        'chainage_pic', 'frame_number', 'event_name', 'name_key', 'run_code'\n",
    "    ]\n",
    "\n",
    "    selected_columns_dis = [col for col in selected_columns_dis if col in processed_dis.columns]\n",
    "    processed_dis[selected_columns_dis].to_csv('access_distress_pic.csv', index=False)\n",
    "    \n",
    "\n",
    "    def process_key(df):\n",
    "        df['event_str'] = round(df['numb_start'] * (df['max_chainage'] / df['pic_count']))\n",
    "        df['event_end'] = round(df['numb_end'] * (df['max_chainage'] / df['pic_count']))\n",
    "        df['event_num'] = df['event_name'].str[0].str.lower()\n",
    "        df['event_type'] = 'pave type'\n",
    "        df['event_name'] = df['event_name'].str.lower()\n",
    "        df['link_id'] = df['linkid']\n",
    "        df['lane_no'] = df['linkid'].apply(lambda x: x[11:13])\n",
    "        df['survey_date'] = df['date']\n",
    "        df['lat_str'] = df.groupby(['survey_code', 'linkid'])['rut_point_x'].transform('first')\n",
    "        df['lat_end'] = df.groupby(['survey_code', 'linkid'])['rut_point_x'].transform('last')\n",
    "        df['lon_str'] = df.groupby(['survey_code', 'linkid'])['rut_point_y'].transform('first')\n",
    "        df['lon_end'] = df.groupby(['survey_code', 'linkid'])['rut_point_y'].transform('last')\n",
    "        df['name_key'] = df['survey_code'].str.replace(r'RUN0*(\\d+)', r'_\\1', regex=True)\n",
    "        df['run_code'] = df['name_key'].str.split('_').str[-1]\n",
    "        \n",
    "        return df\n",
    "\n",
    "    processed_key = merged_df.groupby('survey_code', group_keys=False).apply(process_key).reset_index(drop=True)\n",
    "    processed_key = processed_key.groupby(['linkid', 'survey_date']).first().reset_index()\n",
    "\n",
    "    selected_columns_key = [\n",
    "        'event_str', 'event_end', 'event_num', 'event_type', 'event_name', 'link_id', 'section_id', \n",
    "        'km_start', 'km_end', 'length', 'lane_no', 'survey_date', 'lat_str', 'lat_end', 'lon_str', \n",
    "        'lon_end', 'name_key', 'run_code'\n",
    "    ]\n",
    "\n",
    "    selected_columns_key = [col for col in selected_columns_key if col in processed_key.columns]\n",
    "    processed_key[selected_columns_key].sort_values(by=['run_code', 'event_str', 'event_end'], ascending=[True, True, False]).to_csv('access_key.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_val[selected_columns_val].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert .csv to .mdb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "log = r'D:\\xenomatixs\\output\\survey_data_20240726\\Output'\n",
    "\n",
    "def find_csv_files(start_dir, prefix='log_'):\n",
    "    csv_files = []\n",
    "    found_filenames = []\n",
    "    for dirpath, dirnames, filenames in os.walk(start_dir):\n",
    "        for filename in fnmatch.filter(filenames, f'{prefix}*.xlsx'):\n",
    "            csv_files.append(os.path.join(dirpath, filename))\n",
    "            found_filenames.append(filename)  # Store the filenames\n",
    "    return csv_files, found_filenames\n",
    "\n",
    "folder_names = [name for name in os.listdir(log) if os.path.isdir(os.path.join(log, name))]\n",
    "log_csv_files, filenames = find_csv_files(log)\n",
    "\n",
    "if log_csv_files:\n",
    "    log_df = pd.read_excel(log_csv_files[0])\n",
    "\n",
    "    # Rename columns and clean up column names\n",
    "    log_df.rename(columns={'ผิว': 'event_name', 'link_id ระบบ': 'section_id'}, inplace=True)\n",
    "    log_df.columns = log_df.columns.str.strip()\n",
    "\n",
    "    for folder_name in folder_names:\n",
    "        print(f\"Processing folder: {folder_name}\")\n",
    "\n",
    "        # Perform the initial merge and filter rows where frame_num is between numb_start and numb_end\n",
    "        merged_df = pd.merge(final_df, log_df, how='left', on=['survey_code'], suffixes=('_final_df', '_log_df'))\n",
    "        merged_df = merged_df[(merged_df['frame_num'] >= merged_df['numb_start']) & \n",
    "                              (merged_df['frame_num'] <= merged_df['numb_end'])]\n",
    "        \n",
    "        filtered_df = merged_df[merged_df['survey_code'] == folder_name]\n",
    "        run_code = folder_name.replace(r'RUN0*(\\d+)', r'_\\1', regex=True)\n",
    "        \n",
    "        def mdb_video_process(df):\n",
    "            df['CHAINAGE'] = df['chainage_pic']\n",
    "            df['LRP_OFFSET'] = df['chainage_pic']\n",
    "            df['LRP_NUMBER'] = 0\n",
    "            df['FRAME'] = df['frame_num']\n",
    "            df['GPS_TIME'] = 0\n",
    "            df['X'] = df['rut_point_y']\n",
    "            df['Y'] = df['rut_point_x']\n",
    "            df['Z'] = 0\n",
    "            df['HEADING'] = 0\n",
    "            df['PITCH'] = 0\n",
    "            df['ROLL'] = 0\n",
    "\n",
    "            return df\n",
    "\n",
    "        video_process = mdb_video_process(filtered_df)\n",
    "        \n",
    "        selected_mdb_video_process = [\n",
    "            'CHAINAGE', 'LRP_OFFSET', 'LRP_NUMBER', 'FRAME', 'GPS_TIME', \n",
    "            'X', 'Y', 'Z', 'HEADING', 'PITCH', 'ROLL'\n",
    "        ]\n",
    "\n",
    "        selected_mdb_video_process = [col for col in selected_mdb_video_process if col in video_process.columns]\n",
    "        mdb_video_process_filename = f'D:/xeno/mdb/Video_Processed_{run_code}_2.csv'\n",
    "        video_process[selected_mdb_video_process].to_csv(mdb_video_process_filename, index=False)\n",
    "        \n",
    "        mdb_video_header = pd.DataFrame({\n",
    "            'CAMERA': [1, 2],\n",
    "            'NAME': ['ROW-0', 'PAVE-0'],\n",
    "            'DEVICE': ['XENO', 'XENO'],\n",
    "            'SERIAL': ['6394983', '6394984'],\n",
    "            'INTERVAL': [5, 2],\n",
    "            'WIDTH': [0, 0],\n",
    "            'HEIGHT': [0, 0],\n",
    "            'FRAME_RATE': [0, 0],\n",
    "            'FORMAT': ['422 YUV 8', 'Mono 8'],\n",
    "            'X_SCALE': [0, 0.5],\n",
    "            'Y_SCALE': [0, 0.5],\n",
    "            'DATA_FORMAT': [-1, -1],\n",
    "            'PROCESSING_METHOD': [-1, -1],\n",
    "            'ENABLE_MOBILE_MAPPING': [True, False],\n",
    "            'DISP_PITCH': [0, 0],\n",
    "            'DISP_ROLL': [0, 0],\n",
    "            'DISP_YAW': [0, 0],\n",
    "            'DISP_X': [0, 0],\n",
    "            'DISP_Y': [0, 0],\n",
    "            'DISP_Z': [0, 0],\n",
    "            'HFOV': [0, 0],\n",
    "            'VFOV': [0, 0]\n",
    "        })\n",
    "\n",
    "        mdb_video_header_filename = f'D:/xeno/mdb/Video_Header_{run_code}.csv'\n",
    "        mdb_video_header.to_csv(mdb_video_header_filename, index=False)\n",
    "        \n",
    "        def mdb_survey_header(df):\n",
    "            current_datetime = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            df['SURVEY_ID'] = run_code\n",
    "            df['SURVEY_FILE'] = run_code\n",
    "            df['SURVEY_DESC'] = None\n",
    "            df['SURVEY_DATE'] = current_datetime\n",
    "            df['VEHICLE'] = 'ISS'\n",
    "            df['OPERATOR'] = 'ISS'\n",
    "            df['USER_1_NAME'] = None\n",
    "            df['USER_1'] = None\n",
    "            df['USER_2_NAME'] = None\n",
    "            df['USER_2'] = None\n",
    "            df['USER_3_NAME'] = None\n",
    "            df['USER_3'] = None\n",
    "            df['LRP_FILE'] = f'LRP_{run_code}'\n",
    "            df['LRP_RESET'] = 'N'\n",
    "            df['LRP_START'] = 0\n",
    "            df['CHAIN_INIT'] = 0\n",
    "            df['CHAIN_START'] = 0\n",
    "            df['CHAIN_END'] = round(df['numb_end'] * (df['max_chainage'] / df['pic_count']))\n",
    "            df['SECT_LEN'] = 0\n",
    "            df['DIR'] = 'I'\n",
    "            df['LANE'] = 1\n",
    "            df['DEVICES'] = 'GPS-Geo-DR,LP_V3-LWP,LP_V3-RWP,TPL,Video'\n",
    "            df['OTHERSIDE'] = True\n",
    "            df['VERSION'] = '2.7.3.4/2.7.3.4'\n",
    "            df['MEMO'] = None\n",
    "            df['LENGTH'] = round(df['numb_end'] * (df['max_chainage'] / df['pic_count']))\n",
    "\n",
    "            df = df.astype({\n",
    "                'SURVEY_DATE': 'datetime64[ns]',\n",
    "                'LRP_START': 'int',\n",
    "                'CHAIN_INIT': 'int',\n",
    "                'CHAIN_START': 'int',\n",
    "                'CHAIN_END': 'int',\n",
    "                'SECT_LEN': 'int',\n",
    "                'LANE': 'int',\n",
    "                'OTHERSIDE': 'bool',\n",
    "                'LENGTH': 'int'\n",
    "            })\n",
    "            \n",
    "            return df\n",
    "\n",
    "        survey_header = mdb_survey_header(filtered_df)\n",
    "        survey_header = survey_header.groupby(['SURVEY_ID']).first().reset_index()\n",
    "        \n",
    "        selected_mdb_survey_header = [\n",
    "            'SURVEY_ID', 'SURVEY_FILE', 'SURVEY_DESC', 'SURVEY_DATE', 'VEHICLE', 'OPERATOR', 'USER_1_NAME', 'USER_1', \n",
    "            'USER_2_NAME', 'USER_2', 'USER_3_NAME', 'USER_3', 'LRP_FILE', 'LRP_RESET', 'LRP_START', 'CHAIN_INIT', \n",
    "            'CHAIN_START','CHAIN_END', 'SECT_LEN', 'DIR', 'LANE', 'DEVICES', 'OTHERSIDE', 'VERSION', 'MEMO', 'LENGTH'\n",
    "        ]\n",
    "\n",
    "        selected_mdb_survey_header = [col for col in selected_mdb_survey_header if col in survey_header.columns]\n",
    "        mdb_survey_header_filename = f'D:/xeno/mdb/Survey_Header_{run_code}.csv'\n",
    "        survey_header[selected_mdb_survey_header].to_csv(mdb_survey_header_filename, index=False)\n",
    "        \n",
    "        def mdb_KeyCode_Raw(df):\n",
    "            df['CHAINAGE_START'] = round(df['numb_start'] * (df['max_chainage'] / df['pic_count']))\n",
    "            df['CHAINAGE_END'] = round(df['numb_end'] * (df['max_chainage'] / df['pic_count']))\n",
    "            df['EVENT'] = df['event_name'].str[0].str.lower()\n",
    "            df['SWITCH_GROUP'] = 'pave type.'\n",
    "            df['EVENT_DESC'] = df['event_name'].str.lower()\n",
    "            df['LATITUDE_START'] = df.groupby(['survey_code', 'linkid'])['rut_point_x'].transform('first')\n",
    "            df['LATITUDE_END'] = df.groupby(['survey_code', 'linkid'])['rut_point_x'].transform('last')\n",
    "            df['LONGITUDE_START'] = df.groupby(['survey_code', 'linkid'])['rut_point_y'].transform('first')\n",
    "            df['LONGITUDE_END'] = df.groupby(['survey_code', 'linkid'])['rut_point_y'].transform('last')\n",
    "            df['link_id'] = df['linkid']\n",
    "            df['section_id'] = df['section_id']\n",
    "            df['km_start'] = df['km_start']\n",
    "            df['km_end'] = df['km_end']\n",
    "            df['length'] = df['length']\n",
    "            df['lane_no'] = df['linkid'].apply(lambda x: x[11:13])\n",
    "            df['survey_date'] = df['date']\n",
    "            \n",
    "            return df\n",
    "\n",
    "        KeyCode_Raw = merged_df.groupby('survey_code', group_keys=False).apply(mdb_KeyCode_Raw).reset_index(drop=True)\n",
    "        KeyCode_Raw = KeyCode_Raw.groupby(['linkid', 'survey_date']).first().reset_index()\n",
    "        KeyCode_Raw = KeyCode_Raw[KeyCode_Raw['survey_code'] == folder_name]\n",
    "\n",
    "        selected_mdb_KeyCode_Raw = [\n",
    "            'CHAINAGE_START', 'CHAINAGE_END', 'EVENT', 'SWITCH_GROUP', 'EVENT_DESC', 'LATITUDE_START', 'LATITUDE_END', \n",
    "            'LONGITUDE_START', 'LONGITUDE_END', 'link_id', 'section_id', 'km_start', 'km_end', 'length', 'lane_no', \n",
    "            'survey_date'\n",
    "        ]\n",
    "\n",
    "        selected_mdb_KeyCode_Raw = [col for col in selected_mdb_KeyCode_Raw if col in KeyCode_Raw.columns]\n",
    "        KeyCode_Raw[selected_mdb_KeyCode_Raw].sort_values(by=['lane_no', 'CHAINAGE_START', 'CHAINAGE_END'], ascending=[True, True, False]).to_csv(f'D:/xeno/mdb/KeyCode_Raw_{run_code}.csv', index=False)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert .csv to .mdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyodbc\n",
    "import win32com.client\n",
    "\n",
    "def create_access_db(db_path):\n",
    "    if os.path.isfile(db_path):\n",
    "        print(f\"File already exists: {db_path}\")\n",
    "    else:\n",
    "        access_app = win32com.client.Dispatch(\"Access.Application\")\n",
    "        access_app.NewCurrentDatabase(db_path)\n",
    "        print(f\"Created new Access database at: {db_path}\")\n",
    "        access_app.Quit()\n",
    "\n",
    "def insert_csv_to_access(csv_path, table_name, access_db_path):\n",
    "    conn_str = r\"DRIVER={{Microsoft Access Driver (*.mdb, *.accdb)}};DBQ={};\".format(access_db_path)\n",
    "    con = None\n",
    "    try:\n",
    "        con = pyodbc.connect(conn_str)\n",
    "        con.autocommit = False  # Turn off auto-commit for better performance\n",
    "        cur = con.cursor()\n",
    "\n",
    "        # Start timing the operation\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "\n",
    "        strSQL = (f\"SELECT * INTO [{table_name}] \"\n",
    "                  f\"FROM [text;HDR=Yes;FMT=Delimited(,);Database={os.path.dirname(csv_path)}].{os.path.basename(csv_path)};\")\n",
    "        cur.execute(strSQL)\n",
    "        \n",
    "        con.commit()  # Commit the transaction after all operations\n",
    "        \n",
    "        # End timing and log the time taken\n",
    "        end_time = time.time()\n",
    "        print(f\"Inserted table {table_name} in {end_time - start_time:.2f} seconds.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        if con:\n",
    "            con.rollback()\n",
    "    finally:\n",
    "        if con:\n",
    "            con.close()\n",
    "\n",
    "\n",
    "\n",
    "log = r'D:\\xenomatixs\\output\\survey_data_20240726\\Output'\n",
    "patmdb = r'D:\\xeno\\mdb'\n",
    "\n",
    "folder_names = [name for name in os.listdir(log) if os.path.isdir(os.path.join(log, name))]\n",
    "\n",
    "for folder_name in folder_names:\n",
    "    run_code = folder_name.replace(r'RUN0*(\\d+)', r'_\\1', regex=True)\n",
    "    mdb_path = os.path.join(patmdb, f'{run_code}_edit.mdb')\n",
    "    create_access_db(mdb_path)\n",
    "\n",
    "    # Define CSV files and corresponding table names\n",
    "    csv_files = {\n",
    "        f'KeyCode_Raw_{run_code}.csv': f'KeyCode_Raw_{run_code}', \n",
    "        # f'Survey_Header_{run_code}.csv': f'Survey_Header_{run_code}',\n",
    "        f'Survey_Header_{run_code}.csv': f'Survey_Header',\n",
    "        f'Video_Header_{run_code}.csv': f'Video_Header_{run_code}',\n",
    "        f'Video_Processed_{run_code}_2.csv': f'Video_Processed_{run_code}_2'\n",
    "    }\n",
    "\n",
    "    for csv_name, table_name in csv_files.items():\n",
    "        csv_path = os.path.join(patmdb, csv_name)\n",
    "        \n",
    "        # Insert cleaned CSV into Access database\n",
    "        insert_csv_to_access(csv_path, table_name, mdb_path)\n",
    "        # os.remove(csv_path)\n",
    "        \n",
    "print(f\"All CSV files have been processed and imported into the Access database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D:\\\\xenomatixs\\\\output\\\\survey_data_20240726\\\\Output\\\\log_xenomatix_20240726.xlsx']\n",
      "D:\\xenomatixs\\output\\survey_data_20240726\\Output\\log_xenomatix_20240726.xlsx\n"
     ]
    }
   ],
   "source": [
    "def find_csv_files(start_dir, prefix='log_'):\n",
    "    csv_files = []\n",
    "    for dirpath, dirnames, filenames in os.walk(start_dir):\n",
    "        for filename in fnmatch.filter(filenames, f'{prefix}*.xlsx'):\n",
    "            csv_files.append(os.path.join(dirpath, filename))\n",
    "    return csv_files\n",
    "\n",
    "base_dir = r\"D:\\xenomatixs\"\n",
    "output_dir = os.path.join(base_dir, \"output\")\n",
    "\n",
    "for survey_date in os.listdir(output_dir):\n",
    "    path = os.path.join(output_dir, survey_date, 'Output')\n",
    "    pic = os.path.join(output_dir, survey_date, 'PAVE')\n",
    "    mdb = os.path.join(output_dir, survey_date, 'Data')\n",
    "    \n",
    "    log_csv_files = find_csv_files(path)\n",
    "    print(log_csv_files)\n",
    "    \n",
    "    for file in log_csv_files:\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D:\\\\xenomatixs\\\\output\\\\survey_data_20240726\\\\Output\\\\log_xenomatix_20240726.xlsx']\n",
      "D:\\xenomatixs\\output\\survey_data_20240726\\Output\\log_xenomatix_20240726.xlsx\n"
     ]
    }
   ],
   "source": [
    "log = r'D:\\xenomatixs\\output\\survey_data_20240726\\Output'\n",
    "\n",
    "def find_csv_files(start_dir, prefix='log_'):\n",
    "    csv_files = []\n",
    "    for dirpath, dirnames, filenames in os.walk(start_dir):\n",
    "        for filename in fnmatch.filter(filenames, f'{prefix}*.xlsx'):\n",
    "            csv_files.append(os.path.join(dirpath, filename))\n",
    "    return csv_files\n",
    "\n",
    "# Find all .csv files that start with 'log_'\n",
    "log_csv_files = find_csv_files(path)\n",
    "print(log_csv_files)\n",
    "\n",
    "# Print the list of found files\n",
    "for file in log_csv_files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pyodbc\n",
    "import fnmatch\n",
    "import pandas as pd\n",
    "import win32com.client\n",
    "from datetime import datetime\n",
    "\n",
    "def find_csv_files(start_dir, prefix='log_'):\n",
    "    csv_files = []\n",
    "    for dirpath, dirnames, filenames in os.walk(start_dir):\n",
    "        for filename in fnmatch.filter(filenames, f'{prefix}*.xlsx'):\n",
    "            csv_files.append(os.path.join(dirpath, filename))\n",
    "    return csv_files\n",
    "\n",
    "base_dir = r\"D:\\xenomatixs\"\n",
    "output_dir = os.path.join(base_dir, \"output\")\n",
    "\n",
    "for survey_date in os.listdir(output_dir):\n",
    "    path = os.path.join(output_dir, survey_date, 'Output')\n",
    "    pic = os.path.join(output_dir, survey_date, 'PAVE')\n",
    "    mdb = os.path.join(output_dir, survey_date, 'Data')\n",
    "    \n",
    "    log_csv_files = find_csv_files(path)\n",
    "    if log_csv_files:\n",
    "        log_df = pd.read_excel(log_csv_files[0])\n",
    "        log_df.rename(columns={'ผิว': 'event_name', 'link_id ระบบ': 'section_id'}, inplace=True)\n",
    "        log_df.columns = log_df.columns.str.strip()\n",
    "\n",
    "        folder_names = [name for name in os.listdir(path) if os.path.isdir(os.path.join(path, name))]\n",
    "        for folder_name in folder_names:\n",
    "            print(f\"Processing folder: {folder_name}\")\n",
    "            \n",
    "            # Perform the initial merge and filter rows where frame_num is between numb_start and numb_end\n",
    "            merged_df = pd.merge(final_df, log_df, how='left', on=['survey_code'], suffixes=('_final_df', '_log_df'))\n",
    "            merged_df = merged_df[(merged_df['frame_num'] >= merged_df['numb_start']) & \n",
    "                                (merged_df['frame_num'] <= merged_df['numb_end'])]\n",
    "            \n",
    "            filtered_df = merged_df[merged_df['survey_code'] == folder_name]\n",
    "            run_code = re.sub(r'RUN0*(\\d+)', r'_\\1', folder_name)\n",
    "\n",
    "            def process_val(df):\n",
    "                df['chainage'] = df['chainage_pic']\n",
    "                df['lon'] = df['rut_point_y']\n",
    "                df['lat'] = df['rut_point_x']\n",
    "                df['iri_right'] = df['iri right (m/km)']\n",
    "                df['iri_left'] = df['iri left (m/km)']\n",
    "                df['iri'] = df['iri']\n",
    "                df['iri_lane'] = df['iri_lane']\n",
    "                df['rutt_right'] = df['right_rutting']\n",
    "                df['rutt_left'] = df['left_rutting']\n",
    "                df['rutting'] = df['avg_rutting']\n",
    "                df['texture'] = 0\n",
    "                df['etd_texture'] = 0\n",
    "                df['event_name'] = df['event_name'].str.lower()\n",
    "                df['frame_number'] = df['frame_num']\n",
    "                df['file_name'] = df['survey_code'].str.replace(r'RUN0*(\\d+)', r'_\\1', regex=True)\n",
    "                df['run_code'] = df['file_name'].str.split('_').str[-1]\n",
    "\n",
    "                return df\n",
    "\n",
    "            processed_val = process_val(merged_df)\n",
    "\n",
    "            selected_columns_val = [\n",
    "                'chainage', 'lon', 'lat', 'iri_right', 'iri_left', 'iri', 'iri_lane', 'rutt_right', 'rutt_left', \n",
    "                'rutting', 'texture', 'etd_texture', 'event_name', 'frame_number', 'file_name', 'run_code'\n",
    "            ]\n",
    "\n",
    "            selected_columns_val = [col for col in selected_columns_val if col in processed_val.columns]\n",
    "            processed_val_filename = os.path.join(path, 'access_valuelaser.csv')\n",
    "            processed_val[selected_columns_val].to_csv(os.path.join(processed_val_filename), index=False)\n",
    "            \n",
    "            def process_dis(df):\n",
    "                df['chainage_pic'] = df['chainage_pic']\n",
    "                df['frame_number'] = df['frame_num']\n",
    "                df['event_name'] = df['event_name'].str.lower()\n",
    "                df['name_key'] = df['survey_code'].str.replace(r'RUN0*(\\d+)', r'_\\1', regex=True)\n",
    "                df['run_code'] = df['file_name'].str.split('_').str[-1]\n",
    "\n",
    "                return df\n",
    "\n",
    "            processed_dis = process_dis(merged_df)\n",
    "\n",
    "            selected_columns_dis = [\n",
    "                'chainage_pic', 'frame_number', 'event_name', 'name_key', 'run_code'\n",
    "            ]\n",
    "\n",
    "            selected_columns_dis = [col for col in selected_columns_dis if col in processed_dis.columns]\n",
    "            processed_dis_filename = os.path.join(path, 'access_distress_pic.csv')\n",
    "            processed_dis[selected_columns_dis].to_csv(os.path.join(processed_dis_filename), index=False)\n",
    "            \n",
    "            def process_key(df):\n",
    "                df['event_str'] = round(df['numb_start'] * (df['max_chainage'] / df['pic_count']))\n",
    "                df['event_end'] = round(df['numb_end'] * (df['max_chainage'] / df['pic_count']))\n",
    "                df['event_num'] = df['event_name'].str[0].str.lower()\n",
    "                df['event_type'] = 'pave type'\n",
    "                df['event_name'] = df['event_name'].str.lower()\n",
    "                df['link_id'] = df['linkid']\n",
    "                df['lane_no'] = df['linkid'].apply(lambda x: x[11:13])\n",
    "                df['survey_date'] = df['date']\n",
    "                df['lat_str'] = df.groupby(['survey_code', 'linkid'])['rut_point_x'].transform('first')\n",
    "                df['lat_end'] = df.groupby(['survey_code', 'linkid'])['rut_point_x'].transform('last')\n",
    "                df['lon_str'] = df.groupby(['survey_code', 'linkid'])['rut_point_y'].transform('first')\n",
    "                df['lon_end'] = df.groupby(['survey_code', 'linkid'])['rut_point_y'].transform('last')\n",
    "                df['name_key'] = df['survey_code'].str.replace(r'RUN0*(\\d+)', r'_\\1', regex=True)\n",
    "                df['run_code'] = df['name_key'].str.split('_').str[-1]\n",
    "                \n",
    "                return df\n",
    "\n",
    "            processed_key = merged_df.groupby('survey_code', group_keys=False).apply(process_key).reset_index(drop=True)\n",
    "            processed_key = processed_key.groupby(['linkid', 'survey_date']).first().reset_index()\n",
    "\n",
    "            selected_columns_key = [\n",
    "                'event_str', 'event_end', 'event_num', 'event_type', 'event_name', 'link_id', 'section_id', \n",
    "                'km_start', 'km_end', 'length', 'lane_no', 'survey_date', 'lat_str', 'lat_end', 'lon_str', \n",
    "                'lon_end', 'name_key', 'run_code'\n",
    "            ]\n",
    "\n",
    "            selected_columns_key = [col for col in selected_columns_key if col in processed_key.columns]\n",
    "            processed_key_filename = os.path.join(path, 'access_key.csv')\n",
    "            processed_key[selected_columns_key].sort_values(by=['run_code', 'event_str', 'event_end'], ascending=[True, True, False]).to_csv(os.path.join(processed_key_filename), index=False)\n",
    "# .csv\n",
    "\n",
    "# .mdb \n",
    "        for survey_date in os.listdir(output_dir):\n",
    "            survey_data_dir = os.path.join(output_dir, survey_date, 'Data')\n",
    "            \n",
    "            folder_names = [name for name in os.listdir(survey_data_dir) if os.path.isdir(os.path.join(survey_data_dir, name))]\n",
    "\n",
    "            for folder_name in folder_names:\n",
    "                run_code = re.sub(r'RUN0*(\\d+)', r'_\\1', folder_name)\n",
    "                mdb_folder_path = os.path.join(survey_data_dir, folder_name)\n",
    "                mdb_path = os.path.join(mdb_folder_path, f'{run_code}_edit.mdb')\n",
    "            \n",
    "                if not os.path.isdir(survey_data_dir):\n",
    "                    print(f\"Directory not found: {survey_data_dir}\")\n",
    "                    continue\n",
    "                \n",
    "                def mdb_video_process(df):\n",
    "                    df['CHAINAGE'] = df['chainage_pic']\n",
    "                    df['LRP_OFFSET'] = df['chainage_pic']\n",
    "                    df['LRP_NUMBER'] = 0\n",
    "                    df['FRAME'] = df['frame_num']\n",
    "                    df['GPS_TIME'] = 0\n",
    "                    df['X'] = df['rut_point_y']\n",
    "                    df['Y'] = df['rut_point_x']\n",
    "                    df['Z'] = 0\n",
    "                    df['HEADING'] = 0\n",
    "                    df['PITCH'] = 0\n",
    "                    df['ROLL'] = 0\n",
    "\n",
    "                    return df\n",
    "\n",
    "                video_process = mdb_video_process(filtered_df)\n",
    "                \n",
    "                selected_mdb_video_process = [\n",
    "                    'CHAINAGE', 'LRP_OFFSET', 'LRP_NUMBER', 'FRAME', 'GPS_TIME', \n",
    "                    'X', 'Y', 'Z', 'HEADING', 'PITCH', 'ROLL'\n",
    "                ]\n",
    "\n",
    "                selected_mdb_video_process = [col for col in selected_mdb_video_process if col in video_process.columns]\n",
    "                mdb_video_process_filename = os.path.join(mdb_folder_path, f'Video_Processed_{run_code}_2.csv')\n",
    "                video_process[selected_mdb_video_process].to_csv(mdb_video_process_filename, index=False)\n",
    "                \n",
    "                mdb_video_header = pd.DataFrame({\n",
    "                    'CAMERA': [1, 2],\n",
    "                    'NAME': ['ROW-0', 'PAVE-0'],\n",
    "                    'DEVICE': ['XENO', 'XENO'],\n",
    "                    'SERIAL': ['6394983', '6394984'],\n",
    "                    'INTERVAL': [5, 2],\n",
    "                    'WIDTH': [0, 0],\n",
    "                    'HEIGHT': [0, 0],\n",
    "                    'FRAME_RATE': [0, 0],\n",
    "                    'FORMAT': ['422 YUV 8', 'Mono 8'],\n",
    "                    'X_SCALE': [0, 0.5],\n",
    "                    'Y_SCALE': [0, 0.5],\n",
    "                    'DATA_FORMAT': [-1, -1],\n",
    "                    'PROCESSING_METHOD': [-1, -1],\n",
    "                    'ENABLE_MOBILE_MAPPING': [True, False],\n",
    "                    'DISP_PITCH': [0, 0],\n",
    "                    'DISP_ROLL': [0, 0],\n",
    "                    'DISP_YAW': [0, 0],\n",
    "                    'DISP_X': [0, 0],\n",
    "                    'DISP_Y': [0, 0],\n",
    "                    'DISP_Z': [0, 0],\n",
    "                    'HFOV': [0, 0],\n",
    "                    'VFOV': [0, 0]\n",
    "                })\n",
    "                \n",
    "                mdb_video_header_filename = os.path.join(mdb_folder_path, f'Video_Header_{run_code}.csv')\n",
    "                mdb_video_header.to_csv(mdb_video_header_filename, index=False)\n",
    "                \n",
    "                def mdb_survey_header(df):\n",
    "                    current_datetime = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    df['SURVEY_ID'] = run_code\n",
    "                    df['SURVEY_FILE'] = run_code\n",
    "                    df['SURVEY_DESC'] = None\n",
    "                    df['SURVEY_DATE'] = current_datetime\n",
    "                    df['VEHICLE'] = 'ISS'\n",
    "                    df['OPERATOR'] = 'ISS'\n",
    "                    df['USER_1_NAME'] = None\n",
    "                    df['USER_1'] = None\n",
    "                    df['USER_2_NAME'] = None\n",
    "                    df['USER_2'] = None\n",
    "                    df['USER_3_NAME'] = None\n",
    "                    df['USER_3'] = None\n",
    "                    df['LRP_FILE'] = f'LRP_{run_code}'\n",
    "                    df['LRP_RESET'] = 'N'\n",
    "                    df['LRP_START'] = 0\n",
    "                    df['CHAIN_INIT'] = 0\n",
    "                    df['CHAIN_START'] = 0\n",
    "                    df['CHAIN_END'] = round(df['numb_end'] * (df['max_chainage'] / df['pic_count'])).max()\n",
    "                    df['SECT_LEN'] = 0\n",
    "                    df['DIR'] = 'I'\n",
    "                    df['LANE'] = 1\n",
    "                    df['DEVICES'] = 'GPS-Geo-DR,LP_V3-LWP,LP_V3-RWP,TPL,Video'\n",
    "                    df['OTHERSIDE'] = True\n",
    "                    df['VERSION'] = '2.7.3.4/2.7.3.4'\n",
    "                    df['MEMO'] = None\n",
    "                    df['LENGTH'] = round(df['numb_end'] * (df['max_chainage'] / df['pic_count'])).max()\n",
    "\n",
    "                    df = df.astype({\n",
    "                        'SURVEY_DATE': 'datetime64[ns]',\n",
    "                        'LRP_START': 'int',\n",
    "                        'CHAIN_INIT': 'int',\n",
    "                        'CHAIN_START': 'int',\n",
    "                        'CHAIN_END': 'int',\n",
    "                        'SECT_LEN': 'int',\n",
    "                        'LANE': 'int',\n",
    "                        'OTHERSIDE': 'bool',\n",
    "                        'LENGTH': 'int'\n",
    "                    })\n",
    "                    \n",
    "                    return df\n",
    "\n",
    "                survey_header = mdb_survey_header(filtered_df)\n",
    "                survey_header = survey_header.groupby(['SURVEY_ID']).first().reset_index()\n",
    "                \n",
    "                selected_mdb_survey_header = [\n",
    "                    'SURVEY_ID', 'SURVEY_FILE', 'SURVEY_DESC', 'SURVEY_DATE', 'VEHICLE', 'OPERATOR', 'USER_1_NAME', 'USER_1', \n",
    "                    'USER_2_NAME', 'USER_2', 'USER_3_NAME', 'USER_3', 'LRP_FILE', 'LRP_RESET', 'LRP_START', 'CHAIN_INIT', \n",
    "                    'CHAIN_START','CHAIN_END', 'SECT_LEN', 'DIR', 'LANE', 'DEVICES', 'OTHERSIDE', 'VERSION', 'MEMO', 'LENGTH'\n",
    "                ]\n",
    "\n",
    "                selected_mdb_survey_header = [col for col in selected_mdb_survey_header if col in survey_header.columns]\n",
    "                mdb_survey_header_filename = os.path.join(mdb_folder_path, f'Survey_Header_{run_code}.csv')\n",
    "                survey_header[selected_mdb_survey_header].to_csv(mdb_survey_header_filename, index=False)\n",
    "                \n",
    "                def mdb_KeyCode_Raw(df):\n",
    "                    df['CHAINAGE_START'] = round(df['numb_start'] * (df['max_chainage'] / df['pic_count']))\n",
    "                    df['CHAINAGE_END'] = round(df['numb_end'] * (df['max_chainage'] / df['pic_count']))\n",
    "                    df['EVENT'] = df['event_name'].str[0].str.lower()\n",
    "                    df['SWITCH_GROUP'] = 'pave type.'\n",
    "                    df['EVENT_DESC'] = df['event_name'].str.lower()\n",
    "                    df['LATITUDE_START'] = df.groupby(['survey_code', 'linkid'])['rut_point_x'].transform('first')\n",
    "                    df['LATITUDE_END'] = df.groupby(['survey_code', 'linkid'])['rut_point_x'].transform('last')\n",
    "                    df['LONGITUDE_START'] = df.groupby(['survey_code', 'linkid'])['rut_point_y'].transform('first')\n",
    "                    df['LONGITUDE_END'] = df.groupby(['survey_code', 'linkid'])['rut_point_y'].transform('last')\n",
    "                    df['link_id'] = df['linkid']\n",
    "                    df['section_id'] = df['section_id']\n",
    "                    df['km_start'] = df['km_start']\n",
    "                    df['km_end'] = df['km_end']\n",
    "                    df['length'] = df['length']\n",
    "                    df['lane_no'] = df['linkid'].apply(lambda x: x[11:13])\n",
    "                    df['survey_date'] = df['date']\n",
    "                    \n",
    "                    return df\n",
    "\n",
    "                KeyCode_Raw = merged_df.groupby('survey_code', group_keys=False).apply(mdb_KeyCode_Raw).reset_index(drop=True)\n",
    "                KeyCode_Raw = KeyCode_Raw.groupby(['linkid', 'survey_date']).first().reset_index()\n",
    "                KeyCode_Raw = KeyCode_Raw[KeyCode_Raw['survey_code'] == folder_name]\n",
    "\n",
    "                selected_mdb_KeyCode_Raw = [\n",
    "                    'CHAINAGE_START', 'CHAINAGE_END', 'EVENT', 'SWITCH_GROUP', 'EVENT_DESC', 'LATITUDE_START', 'LATITUDE_END', \n",
    "                    'LONGITUDE_START', 'LONGITUDE_END', 'link_id', 'section_id', 'km_start', 'km_end', 'length', 'lane_no', \n",
    "                    'survey_date'\n",
    "                ]\n",
    "\n",
    "                selected_mdb_KeyCode_Raw = [col for col in selected_mdb_KeyCode_Raw if col in KeyCode_Raw.columns]\n",
    "                mdb_KeyCode_Raw_filename = os.path.join(mdb_folder_path, f'KeyCode_Raw_{run_code}.csv')\n",
    "                KeyCode_Raw[selected_mdb_KeyCode_Raw].sort_values(by=['lane_no', 'CHAINAGE_START', 'CHAINAGE_END'], ascending=[True, True, False]).to_csv(mdb_KeyCode_Raw_filename, index=False)    \n",
    "\n",
    "                # insert .mdb\n",
    "\n",
    "        #         def create_access_db(db_path):\n",
    "        #             if os.path.isfile(db_path):\n",
    "        #                 print(f\"File already exists: {db_path}\")\n",
    "        #             else:\n",
    "        #                 access_app = win32com.client.Dispatch(\"Access.Application\")\n",
    "        #                 access_app.NewCurrentDatabase(db_path)\n",
    "        #                 print(f\"Created new Access database at: {db_path}\")\n",
    "        #                 access_app.Quit()\n",
    "\n",
    "        #         def insert_csv_to_access(csv_path, table_name, access_db_path):\n",
    "        #             conn_str = r\"DRIVER={{Microsoft Access Driver (*.mdb, *.accdb)}};DBQ={};\".format(access_db_path)\n",
    "        #             con = None\n",
    "        #             try:\n",
    "        #                 con = pyodbc.connect(conn_str)\n",
    "        #                 con.autocommit = False  # Turn off auto-commit for better performance\n",
    "        #                 cur = con.cursor()\n",
    "\n",
    "        #                 # Start timing the operation\n",
    "        #                 import time\n",
    "        #                 start_time = time.time()\n",
    "\n",
    "        #                 strSQL = (f\"SELECT * INTO [{table_name}] \"\n",
    "        #                         f\"FROM [text;HDR=Yes;FMT=Delimited(,);Database={os.path.dirname(csv_path)}].{os.path.basename(csv_path)};\")\n",
    "        #                 cur.execute(strSQL)\n",
    "                        \n",
    "        #                 con.commit()  # Commit the transaction after all operations\n",
    "                        \n",
    "        #                 # End timing and log the time taken\n",
    "        #                 end_time = time.time()\n",
    "        #                 print(f\"Inserted table {table_name} in {end_time - start_time:.2f} seconds.\")\n",
    "                        \n",
    "        #             except Exception as e:\n",
    "        #                 print(f\"An error occurred: {e}\")\n",
    "        #                 if con:\n",
    "        #                     con.rollback()\n",
    "        #             finally:\n",
    "        #                 if con:\n",
    "        #                     con.close()\n",
    "                \n",
    "        #         create_access_db(mdb_path)\n",
    "\n",
    "        #         # Define CSV files and corresponding table names\n",
    "        #         csv_files = {\n",
    "        #             f'KeyCode_Raw_{run_code}.csv': f'KeyCode_Raw_{run_code}', \n",
    "        #             f'Survey_Header_{run_code}.csv': f'Survey_Header',\n",
    "        #             f'Video_Header_{run_code}.csv': f'Video_Header_{run_code}',\n",
    "        #             f'Video_Processed_{run_code}_2.csv': f'Video_Processed_{run_code}_2'\n",
    "        #         }\n",
    "\n",
    "        #         for csv_name, table_name in csv_files.items():\n",
    "        #             csv_path = os.path.join(mdb_folder_path, csv_name)\n",
    "                    \n",
    "        #             # Insert CSV into Access database\n",
    "        #             # insert_csv_to_access(csv_path, table_name, mdb_path)\n",
    "        #             # os.remove(csv_path)\n",
    "\n",
    "        # print(\"All CSV files have been processed and imported into the Access database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pyodbc\n",
    "import win32com.client\n",
    "from datetime import datetime\n",
    "\n",
    "base_dir = r\"D:\\xenomatixs\"\n",
    "output_dir = os.path.join(base_dir, \"output\")\n",
    "\n",
    "for survey_date in os.listdir(output_dir):\n",
    "    survey_data_dir = os.path.join(output_dir, survey_date, 'Data')\n",
    "    \n",
    "    if not os.path.isdir(survey_data_dir):\n",
    "        print(f\"Directory not found: {survey_data_dir}\")\n",
    "        continue\n",
    "    \n",
    "    folder_names = [name for name in os.listdir(survey_data_dir) if os.path.isdir(os.path.join(survey_data_dir, name))]\n",
    "\n",
    "    for folder_name in folder_names:\n",
    "        run_code = re.sub(r'RUN0*(\\d+)', r'_\\1', folder_name)\n",
    "        mdb_folder_path = os.path.join(survey_data_dir, folder_name)\n",
    "        mdb_path = os.path.join(mdb_folder_path, f'{run_code}_edit.mdb')\n",
    "\n",
    "        create_access_db(mdb_path)\n",
    "\n",
    "        # Define CSV files and corresponding table names\n",
    "        csv_files = {\n",
    "            f'KeyCode_Raw_{run_code}.csv': f'KeyCode_Raw_{run_code}', \n",
    "            f'Survey_Header_{run_code}.csv': f'Survey_Header',\n",
    "            f'Video_Header_{run_code}.csv': f'Video_Header_{run_code}',\n",
    "            f'Video_Processed_{run_code}_2.csv': f'Video_Processed_{run_code}_2'\n",
    "        }\n",
    "\n",
    "        for csv_name, table_name in csv_files.items():\n",
    "            csv_path = os.path.join(survey_data_dir, csv_name)\n",
    "            \n",
    "            # Insert cleaned CSV into Access database\n",
    "            insert_csv_to_access(csv_path, table_name, mdb_path)\n",
    "\n",
    "print(\"All CSV files have been processed and imported into the Access database.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
