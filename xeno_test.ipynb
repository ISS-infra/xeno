{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import shutil\n",
    "import pyodbc\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import win32com.client\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‡πÅ‡∏ö‡πà‡∏á iri ‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô 4 ‡∏ä‡πà‡∏ß‡∏á (20 -> 5) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤ iri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_parts(target_values, num_parts, tolerance):\n",
    "    parts_list = []\n",
    "    for target_value in target_values:\n",
    "        total_sum = target_value * num_parts\n",
    "        while True:\n",
    "            # Generate random parts\n",
    "            parts = np.random.uniform(low=total_sum / num_parts * 0.9, high=total_sum / num_parts * 1.1, size=num_parts)\n",
    "            # Ensure the sum is correct\n",
    "            if np.abs(np.sum(parts) - total_sum) < tolerance:\n",
    "                parts_list.append(parts)\n",
    "                break\n",
    "    return parts_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‡πÄ‡∏û‡∏¥‡πà‡∏° chainage event_str event_end ‡πÉ‡∏´‡πâ iri/rut return ‡πÄ‡∏õ‡πá‡∏ô dic \n",
    "EX.iri_dataframes['xw_iri_20240726RUN03.csv'] rutting_dataframes['xw_rutting_20240726RUN03.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all relevant CSV files and process them\n",
    "def process_csv_files(path):\n",
    "    iri_dataframes = {}\n",
    "    rutting_dataframes = {}\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        # Find files\n",
    "        iri_files = [f for f in files if f.endswith('.csv') and 'xw_iri_qgis' in f]\n",
    "        rutting_files = [f for f in files if f.endswith('.csv') and 'xw_rutting' in f]\n",
    "        \n",
    "        # Process 'xw_iri_qgis' files\n",
    "        for filename in iri_files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            iri_df = pd.read_csv(file_path, delimiter=';')\n",
    "            iri_df.columns = iri_df.columns.str.strip()\n",
    "            survey_code = filename.split('_')[3].split('.')[0]\n",
    "            iri_df['survey_code'] = survey_code\n",
    "            iri_df['iri'] = (iri_df['iri left (m/km)'] + iri_df['iri right (m/km)']) / 2        \n",
    "            iri_df.drop(columns=['geometry'], errors='ignore', inplace=True)\n",
    "            \n",
    "            # Generate random values\n",
    "            target_values = iri_df['iri']\n",
    "            num_parts = 4\n",
    "            tolerance = 0.3\n",
    "            parts_list = generate_parts(target_values, num_parts, tolerance)\n",
    "\n",
    "            # Expand DataFrame by repeating the rows\n",
    "            iri_df = iri_df.loc[iri_df.index.repeat(num_parts)].reset_index(drop=True)\n",
    "            iri_df['iri_lane'] = np.concatenate(parts_list)\n",
    "            \n",
    "            # Set initial event columns\n",
    "            increment = 5 if fnmatch.fnmatch(filename, '*xw_iri_qgis*') else 5\n",
    "            iri_df['iri_chainage'] = iri_df.index * 5\n",
    "            iri_df['event_start'] = range(0, len(iri_df) * increment, increment)\n",
    "            iri_df['event_end'] = iri_df['event_start'] + increment\n",
    "            iri_dataframes[filename] = iri_df\n",
    "            \n",
    "            # print(f\"Updated {filename} into IRI DataFrame.\")\n",
    "        \n",
    "        # Process 'xw_rutting' files\n",
    "        for filename in rutting_files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            rut_df = pd.read_csv(file_path, delimiter=';')\n",
    "            rut_df.columns = rut_df.columns.str.strip()\n",
    "            if 'Unnamed: 5' in rut_df.columns:\n",
    "                rut_df.drop(columns=['Unnamed: 5'], inplace=True, errors='ignore')\n",
    "            else:\n",
    "                pass\n",
    "            increment = 5 if fnmatch.fnmatch(filename, '*xw_rutting*') else 5\n",
    "            rut_df['event_start'] = range(0, len(rut_df) * increment, increment)\n",
    "            rut_df['event_end'] = rut_df['event_start'] + increment\n",
    "            rut_df['rut_chainage'] = rut_df.index * 25 // 5\n",
    "            survey_code = filename.split('_')[2].split('.')[0]\n",
    "            # rut_df.set_index('index', inplace=True)\n",
    "            rut_df['survey_code'] = survey_code\n",
    "            rut_df['rut_point_x'] = rut_df['qgis_shape'].apply(lambda x: float(x.split('(')[1].split(')')[0].split(',')[0].split(' ')[1]))\n",
    "            rut_df['rut_point_y'] = rut_df['qgis_shape'].apply(lambda x: float(x.split('(')[1].split(')')[0].split(',')[0].split(' ')[0]))\n",
    "            rut_df['rut_point_x'].fillna(0, inplace=True)\n",
    "            rut_df['rut_point_y'].fillna(0, inplace=True)\n",
    "        \n",
    "            rut_df.rename(columns={'#Date':'Date', 'left rutting height': 'left_rutting', 'right rutting height': 'right_rutting', 'average height': 'avg_rutting'}, inplace=True)\n",
    "            rut_df.drop(columns=['qgis_shape'], inplace=True)\n",
    "            rutting_dataframes[filename] = rut_df\n",
    "\n",
    "            # print(f\"Updated {filename} into Rutting DataFrame.\")\n",
    "\n",
    "    print(f\"‚úÖ Finished processing: .CSV files.\")\n",
    "    return iri_dataframes, rutting_dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‡πÄ‡∏û‡∏¥‡πà‡∏° chainage event_str event_end ‡πÉ‡∏´‡πâ iri/rut return ‡πÅ‡∏ö‡∏ö concat ‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ô‡∏ó‡∏µ‡∏•‡∏∞ RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_files(path):\n",
    "    all_iri_dataframes = []  # List to store all IRI dataframes\n",
    "    all_rutting_dataframes = []  # List to store all Rutting dataframes\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        # Find files\n",
    "        iri_files = [f for f in files if f.endswith('.csv') and 'xw_iri_qgis' in f]\n",
    "        rutting_files = [f for f in files if f.endswith('.csv') and 'xw_rutting' in f]\n",
    "\n",
    "        # Process 'xw_iri_qgis' files\n",
    "        for filename in iri_files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            iri_df = pd.read_csv(file_path, delimiter=';')\n",
    "            iri_df.columns = iri_df.columns.str.strip()  # Clean column names\n",
    "            survey_code = filename.split('_')[3].split('.')[0]  # Extract survey code\n",
    "            iri_df['survey_code'] = survey_code\n",
    "            iri_df['iri'] = (iri_df['iri left (m/km)'] + iri_df['iri right (m/km)']) / 2\n",
    "            iri_df.drop(columns=['geometry'], errors='ignore', inplace=True)\n",
    "\n",
    "            # Generate random values for iri_lane\n",
    "            target_values = iri_df['iri']\n",
    "            num_parts = 4\n",
    "            tolerance = 0.3\n",
    "            parts_list = generate_parts(target_values, num_parts, tolerance)\n",
    "\n",
    "            # Expand DataFrame by repeating the rows\n",
    "            iri_df = iri_df.loc[iri_df.index.repeat(num_parts)].reset_index(drop=True)\n",
    "            iri_df['iri_lane'] = np.concatenate(parts_list)\n",
    "\n",
    "            # Set initial event columns\n",
    "            increment = 5\n",
    "            # iri_df['iri_chainage'] = iri_df.index * increment\n",
    "            iri_df['event_start'] = range(0, len(iri_df) * increment, increment)\n",
    "            iri_df['event_end'] = iri_df['event_start'] + increment\n",
    "\n",
    "            # Append the processed IRI DataFrame to the list\n",
    "            all_iri_dataframes.append(iri_df)\n",
    "\n",
    "        # Process 'xw_rutting' files\n",
    "        for filename in rutting_files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            rut_df = pd.read_csv(file_path, delimiter=';')\n",
    "            rut_df.columns = rut_df.columns.str.strip()\n",
    "            if 'Unnamed: 5' in rut_df.columns:\n",
    "                rut_df.drop(columns=['Unnamed: 5'], inplace=True, errors='ignore')\n",
    "            else:\n",
    "                pass\n",
    "            increment = 5 if fnmatch.fnmatch(filename, '*xw_rutting*') else 5\n",
    "            rut_df['event_start'] = range(0, len(rut_df) * increment, increment)\n",
    "            rut_df['event_end'] = rut_df['event_start'] + increment\n",
    "            rut_df['rut_chainage'] = rut_df.index * 25 // 5\n",
    "            survey_code = filename.split('_')[2].split('.')[0]\n",
    "            # rut_df.set_index('index', inplace=True)\n",
    "            rut_df['survey_code'] = survey_code\n",
    "            rut_df['rut_point_x'] = rut_df['qgis_shape'].apply(lambda x: float(x.split('(')[1].split(')')[0].split(',')[0].split(' ')[1]))\n",
    "            rut_df['rut_point_y'] = rut_df['qgis_shape'].apply(lambda x: float(x.split('(')[1].split(')')[0].split(',')[0].split(' ')[0]))\n",
    "            rut_df['rut_point_x'].fillna(0, inplace=True)\n",
    "            rut_df['rut_point_y'].fillna(0, inplace=True)\n",
    "        \n",
    "            rut_df.rename(columns={'#Date':'Date', 'left rutting height': 'left_rutting', 'right rutting height': 'right_rutting', 'average height': 'avg_rutting'}, inplace=True)\n",
    "            rut_df.drop(columns=['qgis_shape'], inplace=True)\n",
    "\n",
    "            all_rutting_dataframes.append(rut_df)\n",
    "\n",
    "    # Concatenate all IRI DataFrames into one\n",
    "    if all_iri_dataframes:\n",
    "        iri_dataframes = pd.concat(all_iri_dataframes, ignore_index=True)\n",
    "    else:\n",
    "        iri_dataframes = pd.DataFrame()  # Empty DataFrame if no IRI data found\n",
    "\n",
    "    # Concatenate all Rutting DataFrames into one\n",
    "    if all_rutting_dataframes:\n",
    "        rutting_dataframes = pd.concat(all_rutting_dataframes, ignore_index=True)\n",
    "    else:\n",
    "        rutting_dataframes = pd.DataFrame()  # Empty DataFrame if no Rutting data found\n",
    "\n",
    "    return iri_dataframes, rutting_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = r'D:\\xenomatix\\output'\n",
    "iri_dataframes, rutting_dataframes = process_csv_files(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "iri_dataframes.to_csv('iri_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rutting_dataframes.to_csv('rut_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "join rut/iri between event_str event_end and survey_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame columns: Index(['Date_rutting', 'left_rutting', 'right_rutting', 'avg_rutting',\n",
      "       'event_start', 'event_end', 'rut_chainage', 'survey_code',\n",
      "       'rut_point_x', 'rut_point_y', 'Date_iri', 'iri left (m/km)',\n",
      "       'iri Std left (m/km)', 'iri right (m/km)', 'iri Std right (m/km)',\n",
      "       'worst iri (m/km)', 'iri difference (m/km)', 'iri', 'iri_lane'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def left_join_dataframes(df_rutting, df_iri):\n",
    "    df_merged = pd.merge(df_rutting, df_iri, how='left', on=['event_start', 'event_end', 'survey_code'], suffixes=('_rutting', '_iri'))\n",
    "    \n",
    "    print(\"Merged DataFrame columns:\", df_merged.columns)\n",
    "\n",
    "    result = df_merged[\n",
    "        (df_merged['rut_chainage'] >= df_merged['event_start']) &\n",
    "        (df_merged['rut_chainage'] < df_merged['event_end'])\n",
    "    ]\n",
    "    return result\n",
    "\n",
    "joined_df = left_join_dataframes(rutting_dataframes, iri_dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‡πÄ‡∏≠‡∏≤ rut/iri ‡∏ó‡∏µ‡πà process csv ‡πÅ‡∏ö‡∏ö concat ‡∏ó‡∏µ‡πà join between ‡πÅ‡∏•‡πâ‡∏ß ‡∏°‡∏≤ join ‡∏Å‡∏±‡∏ö‡∏£‡∏π‡∏õ ‡∏î‡πâ‡∏ß‡∏¢ survey_code ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤ frame_num, frame_num_ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def left_join_dataframes(df_rutting, df_iri):\n",
    "    df_merged = pd.merge(df_rutting, df_iri, how='left', on=['event_start', 'event_end', 'survey_code'], suffixes=('_rutting', '_iri'))\n",
    "    \n",
    "    # print(\"Merged DataFrame columns:\", df_merged.columns)\n",
    "\n",
    "    result = df_merged[\n",
    "        (df_merged['rut_chainage'] >= df_merged['event_start']) &\n",
    "        (df_merged['rut_chainage'] < df_merged['event_end'])\n",
    "    ]\n",
    "    return result\n",
    "\n",
    "def add_frame_num_to_joined_df(joined_df, derived_values, jpg_num):\n",
    "    # Create new columns 'frame_num_ch' and 'frame_num' initialized with NaN\n",
    "    joined_df['frame_ch'] = pd.NA\n",
    "    joined_df['frame_num'] = pd.NA\n",
    "\n",
    "    # Create a DataFrame to map derived values to frame numbers\n",
    "    derived_to_frame_mapping = pd.DataFrame({\n",
    "        'frame_num_ch': derived_values,\n",
    "        'frame_num': jpg_num\n",
    "    })\n",
    "\n",
    "    # Additional logic to update frame_num_ch and frame_num based on derived_values\n",
    "    for i, frame_num_ch in enumerate(derived_values):\n",
    "        mask = (joined_df['event_start'] <= frame_num_ch) & (joined_df['event_end'] > frame_num_ch)\n",
    "        joined_df.loc[mask, 'frame_num_ch'] = frame_num_ch\n",
    "        joined_df.loc[mask, 'frame_num'] = jpg_num[i]\n",
    "    return joined_df\n",
    "\n",
    "def get_jpg_names_and_nums(directory):\n",
    "    jpg_num_dict = {}\n",
    "    jpg_dict = {}\n",
    "    \n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        jpg_files = [f for f in files if f.endswith('.jpg')]\n",
    "        if jpg_files:\n",
    "            folder_name = os.path.basename(os.path.dirname(root))\n",
    "            jpg_dict[folder_name] = len(jpg_files)\n",
    "            jpg_num = [int(files.split('-')[-1].split('.jpg')[0]) for files in files]\n",
    "            jpg_num_dict = jpg_num\n",
    "            \n",
    "    frame_df = pd.DataFrame(list(jpg_dict.items()), columns=['survey_code', 'pic_count'])\n",
    "    frame_df['survey_code'] = frame_df['survey_code'].str.replace(r'_(\\d+)', lambda m: f\"RUN{int(m.group(1)):02d}\", regex=True)\n",
    "    return jpg_files, frame_df, jpg_num\n",
    "\n",
    "# Perform fainal data frame\n",
    "def process_fainal_df(output_dir):\n",
    "    jpg_files, frame_df, jpg_num = get_jpg_names_and_nums(output_dir)\n",
    "    iri_dataframes, rutting_dataframes = process_csv_files(output_dir)\n",
    "\n",
    "    joined_dataframes = []\n",
    "\n",
    "    joined_df = left_join_dataframes(rutting_dataframes, iri_dataframes)\n",
    "    \n",
    "    grouped_df = joined_df.groupby('survey_code').agg(\n",
    "        max_chainage=('rut_chainage', 'max'),\n",
    "        min_chainage=('rut_chainage', 'min')\n",
    "    ).reset_index()\n",
    "\n",
    "    joined_df = pd.merge(joined_df, grouped_df, on='survey_code', how='left')\n",
    "    joined_df = pd.merge(joined_df, frame_df, on='survey_code', how='left')\n",
    "    \n",
    "    max_event_start = joined_df['event_start'].max()\n",
    "    \n",
    "    derived_values = [round((max_event_start * num) / max(jpg_num)) for num in jpg_num]\n",
    "    joined_df = add_frame_num_to_joined_df(joined_df, derived_values, jpg_num)\n",
    "\n",
    "    joined_dataframes.append(joined_df)\n",
    "                \n",
    "    final_df = pd.concat(joined_dataframes, ignore_index=True)\n",
    "    \n",
    "    final_df = final_df.rename(columns={'rut_chainage':'chainage'})\n",
    "    \n",
    "    selected_columns = [\n",
    "        'left_rutting', 'right_rutting', 'avg_rutting', 'event_start', 'event_end', 'survey_code',\n",
    "        'rut_point_x', 'rut_point_y', 'Date', 'iri left (m/km)', 'iri right (m/km)', 'iri', 'iri_lane', \n",
    "        'chainage', 'max_chainage', 'min_chainage', 'pic_count', 'frame_num', 'frame_num_ch'\n",
    "    ] \n",
    "    \n",
    "    selected_columns = [col for col in selected_columns if col in final_df.columns]\n",
    "    final_df = final_df[final_df['Date_rutting'].notnull()][selected_columns]\n",
    "    # final_df = final_df[final_df['iri'].notnull()][selected_columns]\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left_rutting</th>\n",
       "      <th>right_rutting</th>\n",
       "      <th>avg_rutting</th>\n",
       "      <th>event_start</th>\n",
       "      <th>event_end</th>\n",
       "      <th>survey_code</th>\n",
       "      <th>rut_point_x</th>\n",
       "      <th>rut_point_y</th>\n",
       "      <th>iri left (m/km)</th>\n",
       "      <th>iri right (m/km)</th>\n",
       "      <th>iri</th>\n",
       "      <th>iri_lane</th>\n",
       "      <th>chainage</th>\n",
       "      <th>max_chainage</th>\n",
       "      <th>min_chainage</th>\n",
       "      <th>pic_count</th>\n",
       "      <th>frame_num</th>\n",
       "      <th>frame_num_ch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2435</th>\n",
       "      <td>0.26</td>\n",
       "      <td>2.96</td>\n",
       "      <td>1.61</td>\n",
       "      <td>5990</td>\n",
       "      <td>5995</td>\n",
       "      <td>20240726RUN03</td>\n",
       "      <td>13.849395</td>\n",
       "      <td>100.444022</td>\n",
       "      <td>8.690157</td>\n",
       "      <td>10.062783</td>\n",
       "      <td>9.376470</td>\n",
       "      <td>9.387792</td>\n",
       "      <td>5990</td>\n",
       "      <td>6035</td>\n",
       "      <td>0</td>\n",
       "      <td>857</td>\n",
       "      <td>831</td>\n",
       "      <td>5993.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2436</th>\n",
       "      <td>0.41</td>\n",
       "      <td>6.50</td>\n",
       "      <td>3.46</td>\n",
       "      <td>5995</td>\n",
       "      <td>6000</td>\n",
       "      <td>20240726RUN03</td>\n",
       "      <td>13.849415</td>\n",
       "      <td>100.443981</td>\n",
       "      <td>8.690157</td>\n",
       "      <td>10.062783</td>\n",
       "      <td>9.376470</td>\n",
       "      <td>9.644570</td>\n",
       "      <td>5995</td>\n",
       "      <td>6035</td>\n",
       "      <td>0</td>\n",
       "      <td>857</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2437</th>\n",
       "      <td>0.34</td>\n",
       "      <td>6.09</td>\n",
       "      <td>3.22</td>\n",
       "      <td>6000</td>\n",
       "      <td>6005</td>\n",
       "      <td>20240726RUN03</td>\n",
       "      <td>13.849436</td>\n",
       "      <td>100.443940</td>\n",
       "      <td>15.840459</td>\n",
       "      <td>16.280330</td>\n",
       "      <td>16.060394</td>\n",
       "      <td>15.790306</td>\n",
       "      <td>6000</td>\n",
       "      <td>6035</td>\n",
       "      <td>0</td>\n",
       "      <td>857</td>\n",
       "      <td>832</td>\n",
       "      <td>6000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>5.66</td>\n",
       "      <td>2.46</td>\n",
       "      <td>4.06</td>\n",
       "      <td>6005</td>\n",
       "      <td>6010</td>\n",
       "      <td>20240726RUN03</td>\n",
       "      <td>13.849459</td>\n",
       "      <td>100.443900</td>\n",
       "      <td>15.840459</td>\n",
       "      <td>16.280330</td>\n",
       "      <td>16.060394</td>\n",
       "      <td>15.503531</td>\n",
       "      <td>6005</td>\n",
       "      <td>6035</td>\n",
       "      <td>0</td>\n",
       "      <td>857</td>\n",
       "      <td>833</td>\n",
       "      <td>6007.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>6.84</td>\n",
       "      <td>0.27</td>\n",
       "      <td>3.56</td>\n",
       "      <td>6010</td>\n",
       "      <td>6015</td>\n",
       "      <td>20240726RUN03</td>\n",
       "      <td>13.849488</td>\n",
       "      <td>100.443864</td>\n",
       "      <td>15.840459</td>\n",
       "      <td>16.280330</td>\n",
       "      <td>16.060394</td>\n",
       "      <td>17.579181</td>\n",
       "      <td>6010</td>\n",
       "      <td>6035</td>\n",
       "      <td>0</td>\n",
       "      <td>857</td>\n",
       "      <td>834</td>\n",
       "      <td>6014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>1.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.63</td>\n",
       "      <td>6015</td>\n",
       "      <td>6020</td>\n",
       "      <td>20240726RUN03</td>\n",
       "      <td>13.849523</td>\n",
       "      <td>100.443835</td>\n",
       "      <td>15.840459</td>\n",
       "      <td>16.280330</td>\n",
       "      <td>16.060394</td>\n",
       "      <td>15.255463</td>\n",
       "      <td>6015</td>\n",
       "      <td>6035</td>\n",
       "      <td>0</td>\n",
       "      <td>857</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>36.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.05</td>\n",
       "      <td>6020</td>\n",
       "      <td>6025</td>\n",
       "      <td>20240726RUN03</td>\n",
       "      <td>13.849565</td>\n",
       "      <td>100.443818</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6020</td>\n",
       "      <td>6035</td>\n",
       "      <td>0</td>\n",
       "      <td>857</td>\n",
       "      <td>835</td>\n",
       "      <td>6021.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>16.73</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.36</td>\n",
       "      <td>6025</td>\n",
       "      <td>6030</td>\n",
       "      <td>20240726RUN03</td>\n",
       "      <td>13.849609</td>\n",
       "      <td>100.443813</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6025</td>\n",
       "      <td>6035</td>\n",
       "      <td>0</td>\n",
       "      <td>857</td>\n",
       "      <td>836</td>\n",
       "      <td>6029.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2443</th>\n",
       "      <td>1.78</td>\n",
       "      <td>0.81</td>\n",
       "      <td>1.30</td>\n",
       "      <td>6030</td>\n",
       "      <td>6035</td>\n",
       "      <td>20240726RUN03</td>\n",
       "      <td>13.849654</td>\n",
       "      <td>100.443820</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6030</td>\n",
       "      <td>6035</td>\n",
       "      <td>0</td>\n",
       "      <td>857</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2444</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.68</td>\n",
       "      <td>6035</td>\n",
       "      <td>6040</td>\n",
       "      <td>20240726RUN03</td>\n",
       "      <td>13.849698</td>\n",
       "      <td>100.443832</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6035</td>\n",
       "      <td>6035</td>\n",
       "      <td>0</td>\n",
       "      <td>857</td>\n",
       "      <td>837</td>\n",
       "      <td>6036.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      left_rutting  right_rutting  avg_rutting  event_start  event_end  \\\n",
       "2435          0.26           2.96         1.61         5990       5995   \n",
       "2436          0.41           6.50         3.46         5995       6000   \n",
       "2437          0.34           6.09         3.22         6000       6005   \n",
       "2438          5.66           2.46         4.06         6005       6010   \n",
       "2439          6.84           0.27         3.56         6010       6015   \n",
       "2440          1.26           0.00         0.63         6015       6020   \n",
       "2441         36.10           0.00        18.05         6020       6025   \n",
       "2442         16.73           0.00         8.36         6025       6030   \n",
       "2443          1.78           0.81         1.30         6030       6035   \n",
       "2444          0.43           0.92         0.68         6035       6040   \n",
       "\n",
       "        survey_code  rut_point_x  rut_point_y  iri left (m/km)  \\\n",
       "2435  20240726RUN03    13.849395   100.444022         8.690157   \n",
       "2436  20240726RUN03    13.849415   100.443981         8.690157   \n",
       "2437  20240726RUN03    13.849436   100.443940        15.840459   \n",
       "2438  20240726RUN03    13.849459   100.443900        15.840459   \n",
       "2439  20240726RUN03    13.849488   100.443864        15.840459   \n",
       "2440  20240726RUN03    13.849523   100.443835        15.840459   \n",
       "2441  20240726RUN03    13.849565   100.443818              NaN   \n",
       "2442  20240726RUN03    13.849609   100.443813              NaN   \n",
       "2443  20240726RUN03    13.849654   100.443820              NaN   \n",
       "2444  20240726RUN03    13.849698   100.443832              NaN   \n",
       "\n",
       "      iri right (m/km)        iri   iri_lane  chainage  max_chainage  \\\n",
       "2435         10.062783   9.376470   9.387792      5990          6035   \n",
       "2436         10.062783   9.376470   9.644570      5995          6035   \n",
       "2437         16.280330  16.060394  15.790306      6000          6035   \n",
       "2438         16.280330  16.060394  15.503531      6005          6035   \n",
       "2439         16.280330  16.060394  17.579181      6010          6035   \n",
       "2440         16.280330  16.060394  15.255463      6015          6035   \n",
       "2441               NaN        NaN        NaN      6020          6035   \n",
       "2442               NaN        NaN        NaN      6025          6035   \n",
       "2443               NaN        NaN        NaN      6030          6035   \n",
       "2444               NaN        NaN        NaN      6035          6035   \n",
       "\n",
       "      min_chainage  pic_count frame_num  frame_num_ch  \n",
       "2435             0        857       831        5993.0  \n",
       "2436             0        857      <NA>           NaN  \n",
       "2437             0        857       832        6000.0  \n",
       "2438             0        857       833        6007.0  \n",
       "2439             0        857       834        6014.0  \n",
       "2440             0        857      <NA>           NaN  \n",
       "2441             0        857       835        6021.0  \n",
       "2442             0        857       836        6029.0  \n",
       "2443             0        857      <NA>           NaN  \n",
       "2444             0        857       837        6036.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = r'D:\\xenomatix\\output'\n",
    "final_df = process_fainal_df(output_dir)\n",
    "\n",
    "# final_df.to_csv('joinjoin.csv')\n",
    "final_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‡πÄ‡∏≠‡∏≤ rut/iri ‡∏ó‡∏µ‡πà process csv ‡πÅ‡∏ö‡∏ö dic ‡πÅ‡∏•‡∏∞‡∏ó‡∏µ‡πà join ‡πÅ‡∏ö‡∏ö‡∏õ‡∏Å‡∏ï‡∏¥ ‡πÅ‡∏•‡πâ‡∏ß ‡∏°‡∏≤ join ‡∏Å‡∏±‡∏ö‡∏£‡∏π‡∏õ ‡∏î‡πâ‡∏ß‡∏¢ survey_code ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤ frame_num, frame_num_ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the left join on xw_rutting and xw_iri_qgis\n",
    "def left_join_dataframes(df_rutting, df_iri):\n",
    "    joined_df = pd.merge(df_rutting, df_iri, how='left', on=['event_start', 'event_end'], suffixes=('_rutting', '_iri'))\n",
    "    return joined_df\n",
    "\n",
    "def add_frame_num_to_joined_df(joined_df, derived_values, jpg_num):\n",
    "    # Create new columns 'frame_num_ch' and 'frame_num' initialized with NaN\n",
    "    joined_df['frame_ch'] = pd.NA\n",
    "    joined_df['frame_num'] = pd.NA\n",
    "\n",
    "    # Create a DataFrame to map derived values to frame numbers\n",
    "    derived_to_frame_mapping = pd.DataFrame({\n",
    "        'frame_num_ch': derived_values,\n",
    "        'frame_num': jpg_num\n",
    "    })\n",
    "\n",
    "    # Additional logic to update frame_num_ch and frame_num based on derived_values\n",
    "    for i, frame_num_ch in enumerate(derived_values):\n",
    "        mask = (joined_df['event_start'] <= frame_num_ch) & (joined_df['event_end'] > frame_num_ch)\n",
    "        joined_df.loc[mask, 'frame_num_ch'] = frame_num_ch\n",
    "        joined_df.loc[mask, 'frame_num'] = jpg_num[i]\n",
    "    return joined_df\n",
    "\n",
    "def get_jpg_names_and_nums(directory):\n",
    "    jpg_num_dict = {}\n",
    "    jpg_dict = {}\n",
    "    \n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        jpg_files = [f for f in files if f.endswith('.jpg')]\n",
    "        if jpg_files:\n",
    "            folder_name = os.path.basename(os.path.dirname(root))\n",
    "            jpg_dict[folder_name] = len(jpg_files)\n",
    "            jpg_num = [int(files.split('-')[-1].split('.jpg')[0]) for files in files]\n",
    "            jpg_num_dict = jpg_num\n",
    "            \n",
    "    frame_df = pd.DataFrame(list(jpg_dict.items()), columns=['survey_code', 'pic_count'])\n",
    "    frame_df['survey_code'] = frame_df['survey_code'].str.replace(r'_(\\d+)', lambda m: f\"RUN{int(m.group(1)):02d}\", regex=True)\n",
    "    return jpg_files, frame_df, jpg_num\n",
    "\n",
    "# Perform fainal data frame\n",
    "def process_fainal_df(output_dir):\n",
    "    jpg_files, frame_df, jpg_num = get_jpg_names_and_nums(output_dir)\n",
    "    iri_dataframes, rutting_dataframes = process_csv_files(output_dir)\n",
    "\n",
    "    joined_dataframes = {}\n",
    "    for rutting_file in rutting_dataframes:\n",
    "        for iri_file in iri_dataframes:\n",
    "            if 'xw_rutting' in rutting_file and 'xw_iri_qgis' in iri_file:\n",
    "                # joined_df = left_join_dataframes(rutting_dataframes[rutting_file], iri_dataframes[iri_file])\n",
    "                joined_df = left_join_dataframes(rutting_dataframes, iri_dataframes)\n",
    "                \n",
    "                grouped_df = joined_df.groupby('survey_code').agg(\n",
    "                    max_chainage=('chainage', 'max'),\n",
    "                    min_chainage=('chainage', 'min')\n",
    "                ).reset_index()\n",
    "\n",
    "                joined_df = pd.merge(joined_df, grouped_df, on='survey_code', how='left')\n",
    "                joined_df = pd.merge(joined_df, frame_df, on='survey_code', how='left')\n",
    "                \n",
    "                max_event_start = joined_df['event_start'].max()\n",
    "                \n",
    "                derived_values = [round((max_event_start * num) / max(jpg_num)) for num in jpg_num]\n",
    "                joined_df = add_frame_num_to_joined_df(joined_df, derived_values, jpg_num)\n",
    "\n",
    "                # Store the joined DataFrame in the dictionary\n",
    "                joined_dataframes[f\"{rutting_file}_{iri_file}\"] = joined_df\n",
    "                \n",
    "    final_df = pd.concat(joined_dataframes.values(), ignore_index=True)\n",
    "    \n",
    "    selected_columns = [\n",
    "        'left_rutting', 'right_rutting', 'avg_rutting', 'event_start', 'event_end', 'survey_code',\n",
    "        'rut_point_x', 'rut_point_y', 'Date', 'iri left (m/km)', 'iri right (m/km)', 'iri', 'iri_lane', \n",
    "        'chainage', 'max_chainage', 'min_chainage', 'pic_count', 'frame_num', 'frame_num_ch'\n",
    "    ] \n",
    "    \n",
    "    selected_columns = [col for col in selected_columns if col in final_df.columns]\n",
    "    final_df = final_df[final_df['iri'].notnull()][selected_columns]\n",
    "    return final_df\n",
    "\n",
    "# edit 9/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = r'D:\\xenomatix\\output'\n",
    "final_df = process_fainal_df(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‡πÉ‡∏ä‡πâ final_df ‡∏ó‡∏µ‡πà join between ‡πÅ‡∏•‡∏∞ process csv ‡πÅ‡∏ö‡∏ö concat ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏≠‡∏Å csv ‡πÉ‡∏ô function main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_csv_files(start_dir, prefix='log_'):\n",
    "    csv_files = []\n",
    "    for dirpath, dirnames, filenames in os.walk(start_dir):\n",
    "        for filename in fnmatch.filter(filenames, f'{prefix}*.xlsx'):\n",
    "            csv_files.append(os.path.join(dirpath, filename))\n",
    "    return csv_files\n",
    "\n",
    "def main(final_df):\n",
    "    for survey_date in os.listdir(output_dir): # eg. base_dir = r\"D:\\xenomatixs\"\n",
    "        path = os.path.join(output_dir, survey_date, 'Output')\n",
    "        mdb = os.path.join(output_dir, survey_date, 'Data')\n",
    "        \n",
    "        log_csv_files = find_csv_files(path)\n",
    "        if log_csv_files:\n",
    "            log_df = pd.read_excel(log_csv_files[0])\n",
    "            log_df.rename(columns={'‡∏ú‡∏¥‡∏ß': 'event_name', 'link_id ‡∏£‡∏∞‡∏ö‡∏ö': 'section_id'}, inplace=True)\n",
    "            log_df.columns = log_df.columns.str.strip()\n",
    "\n",
    "            folder_names = [name for name in os.listdir(path) if os.path.isdir(os.path.join(path, name))]\n",
    "            for folder_name in folder_names:\n",
    "                print(f\"üîÑ Processing folder: {folder_name}\")\n",
    "                \n",
    "                # Perform the initial merge and filter rows where frame_num is between numb_start and numb_end\n",
    "                merged_df = pd.merge(final_df, log_df, how='left', on=['survey_code'], suffixes=('_final_df', '_log_df'))\n",
    "                print(merged_df.columns)\n",
    "                merged_df = merged_df[(merged_df['frame_num'] >= merged_df['numb_start']) & \n",
    "                                    (merged_df['frame_num'] <= merged_df['numb_end'])]\n",
    "                \n",
    "                merged_df.to_csv('sss.csv')\n",
    "                filtered_df = merged_df[merged_df['survey_code'] == folder_name]\n",
    "                run_code = re.sub(r'RUN0*(\\d+)', r'_\\1', folder_name)\n",
    "\n",
    "                def process_val(df):\n",
    "                    df['chainage'] = df['chainage']\n",
    "                    df['lon'] = df['rut_point_y']\n",
    "                    df['lat'] = df['rut_point_x']\n",
    "                    df['iri_right'] = df['iri right (m/km)']\n",
    "                    df['iri_left'] = df['iri left (m/km)']\n",
    "                    df['iri'] = df['iri']\n",
    "                    df['iri_lane'] = df['iri_lane']\n",
    "                    df['rutt_right'] = df['right_rutting']\n",
    "                    df['rutt_left'] = df['left_rutting']\n",
    "                    df['rutting'] = df['avg_rutting']\n",
    "                    df['texture'] = 0\n",
    "                    df['etd_texture'] = 0\n",
    "                    df['event_name'] = df['event_name'].str.lower()\n",
    "                    df['frame_number'] = df['frame_num']\n",
    "                    df['file_name'] = df['survey_code'].str.replace(r'RUN0*(\\d+)', r'_\\1', regex=True)\n",
    "                    df['run_code'] = df['file_name'].str.split('_').str[-1]\n",
    "\n",
    "                    return df\n",
    "\n",
    "                processed_val = process_val(merged_df)\n",
    "\n",
    "                selected_columns_val = [\n",
    "                    'chainage', 'lon', 'lat', 'iri_right', 'iri_left', 'iri', 'iri_lane', 'rutt_right', 'rutt_left', \n",
    "                    'rutting', 'texture', 'etd_texture', 'event_name', 'frame_number', 'file_name', 'run_code'\n",
    "                ]\n",
    "\n",
    "                selected_columns_val = [col for col in selected_columns_val if col in processed_val.columns]\n",
    "                processed_val_filename = os.path.join(mdb, 'access_valuelaser.csv')\n",
    "                processed_val[selected_columns_val].to_csv(os.path.join(processed_val_filename), index=False)\n",
    "                \n",
    "                def process_dis(df):\n",
    "                    df['chainage_pic'] = df['chainage']\n",
    "                    df['frame_number'] = df['frame_num']\n",
    "                    df['event_name'] = df['event_name'].str.lower()\n",
    "                    df['name_key'] = df['survey_code'].str.replace(r'RUN0*(\\d+)', r'_\\1', regex=True)\n",
    "                    df['run_code'] = df['file_name'].str.split('_').str[-1]\n",
    "\n",
    "                    return df\n",
    "\n",
    "                processed_dis = process_dis(merged_df)\n",
    "\n",
    "                selected_columns_dis = [\n",
    "                    'chainage_pic', 'frame_number', 'event_name', 'name_key', 'run_code'\n",
    "                ]\n",
    "\n",
    "                selected_columns_dis = [col for col in selected_columns_dis if col in processed_dis.columns]\n",
    "                processed_dis_filename = os.path.join(mdb, 'access_distress_pic.csv')\n",
    "                processed_dis[selected_columns_dis].to_csv(os.path.join(processed_dis_filename), index=False)\n",
    "                \n",
    "                def process_key(df):\n",
    "                    df['event_str'] = round(df['numb_start'] * (df['max_chainage'] / df['pic_count']))\n",
    "                    df['event_end'] = round(df['numb_end'] * (df['max_chainage'] / df['pic_count']))\n",
    "                    df['event_num'] = df['event_name'].str[0].str.lower()\n",
    "                    df['event_type'] = 'pave type'\n",
    "                    df['event_name'] = df['event_name'].str.lower()\n",
    "                    df['link_id'] = df['linkid']\n",
    "                    df['lane_no'] = df['linkid'].apply(lambda x: x[11:13])\n",
    "                    df['survey_date'] = df['date']\n",
    "                    df['lat_str'] = df.groupby(['survey_code', 'linkid'])['rut_point_x'].transform('first')\n",
    "                    df['lat_end'] = df.groupby(['survey_code', 'linkid'])['rut_point_x'].transform('last')\n",
    "                    df['lon_str'] = df.groupby(['survey_code', 'linkid'])['rut_point_y'].transform('first')\n",
    "                    df['lon_end'] = df.groupby(['survey_code', 'linkid'])['rut_point_y'].transform('last')\n",
    "                    df['name_key'] = df['survey_code'].str.replace(r'RUN0*(\\d+)', r'_\\1', regex=True)\n",
    "                    df['run_code'] = df['name_key'].str.split('_').str[-1]\n",
    "                    \n",
    "                    return df\n",
    "\n",
    "                processed_key = merged_df.groupby('survey_code', group_keys=False).apply(process_key).reset_index(drop=True)\n",
    "                processed_key = processed_key.groupby(['linkid', 'survey_date']).first().reset_index()\n",
    "\n",
    "                selected_columns_key = [\n",
    "                    'event_str', 'event_end', 'event_num', 'event_type', 'event_name', 'link_id', 'section_id', \n",
    "                    'km_start', 'km_end', 'length', 'lane_no', 'survey_date', 'lat_str', 'lat_end', 'lon_str', \n",
    "                    'lon_end', 'name_key', 'run_code'\n",
    "                ]\n",
    "\n",
    "                selected_columns_key = [col for col in selected_columns_key if col in processed_key.columns]\n",
    "                processed_key_filename = os.path.join(mdb, 'access_key.csv')\n",
    "                processed_key[selected_columns_key].sort_values(by=['run_code', 'event_str', 'event_end'], ascending=[True, True, False]).to_csv(os.path.join(processed_key_filename), index=False)\n",
    "# .csv\n",
    "# .mdb \n",
    "                mdb_folder_path = os.path.join(mdb, run_code)\n",
    "                # print(f'store in: {mdb_folder_path}')\n",
    "                mdb_path = os.path.join(mdb_folder_path, f'{run_code}_edit.mdb')\n",
    "                # print(f'this name: {mdb_path}')\n",
    "            \n",
    "                if not os.path.isdir(mdb):\n",
    "                    print(f\"‚õî Directory not found: {mdb}\")\n",
    "                    continue\n",
    "                \n",
    "                def mdb_video_process(df):\n",
    "                    df['CHAINAGE'] = df['chainage']\n",
    "                    df['LRP_OFFSET'] = df['chainage']\n",
    "                    df['LRP_NUMBER'] = 0\n",
    "                    df['FRAME'] = df['frame_num']\n",
    "                    df['GPS_TIME'] = 0\n",
    "                    df['X'] = df['rut_point_y']\n",
    "                    df['Y'] = df['rut_point_x']\n",
    "                    df['Z'] = 0\n",
    "                    df['HEADING'] = 0\n",
    "                    df['PITCH'] = 0\n",
    "                    df['ROLL'] = 0\n",
    "\n",
    "                    return df\n",
    "\n",
    "                video_process = mdb_video_process(filtered_df)\n",
    "                \n",
    "                selected_mdb_video_process = [\n",
    "                    'CHAINAGE', 'LRP_OFFSET', 'LRP_NUMBER', 'FRAME', 'GPS_TIME', \n",
    "                    'X', 'Y', 'Z', 'HEADING', 'PITCH', 'ROLL'\n",
    "                ]\n",
    "\n",
    "                selected_mdb_video_process = [col for col in selected_mdb_video_process if col in video_process.columns]\n",
    "                mdb_video_process_filename = os.path.join(mdb_folder_path, f'Video_Processed_{run_code}_2.csv')\n",
    "                video_process[selected_mdb_video_process].to_csv(mdb_video_process_filename, index=False)\n",
    "                \n",
    "                mdb_video_header = pd.DataFrame({\n",
    "                    'CAMERA': [1, 2],\n",
    "                    'NAME': ['ROW-0', 'PAVE-0'],\n",
    "                    'DEVICE': ['XENO', 'XENO'],\n",
    "                    'SERIAL': ['6394983', '6394984'],\n",
    "                    'INTERVAL': [5, 2],\n",
    "                    'WIDTH': [0, 0],\n",
    "                    'HEIGHT': [0, 0],\n",
    "                    'FRAME_RATE': [0, 0],\n",
    "                    'FORMAT': ['422 YUV 8', 'Mono 8'],\n",
    "                    'X_SCALE': [0, 0.5],\n",
    "                    'Y_SCALE': [0, 0.5],\n",
    "                    'DATA_FORMAT': [-1, -1],\n",
    "                    'PROCESSING_METHOD': [-1, -1],\n",
    "                    'ENABLE_MOBILE_MAPPING': [True, False],\n",
    "                    'DISP_PITCH': [0, 0],\n",
    "                    'DISP_ROLL': [0, 0],\n",
    "                    'DISP_YAW': [0, 0],\n",
    "                    'DISP_X': [0, 0],\n",
    "                    'DISP_Y': [0, 0],\n",
    "                    'DISP_Z': [0, 0],\n",
    "                    'HFOV': [0, 0],\n",
    "                    'VFOV': [0, 0]\n",
    "                })\n",
    "                \n",
    "                mdb_video_header_filename = os.path.join(mdb_folder_path, f'Video_Header_{run_code}.csv')\n",
    "                mdb_video_header.to_csv(mdb_video_header_filename, index=False)\n",
    "                \n",
    "                def mdb_survey_header(df):\n",
    "                    current_datetime = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    df['SURVEY_ID'] = run_code\n",
    "                    df['SURVEY_FILE'] = run_code\n",
    "                    df['SURVEY_DESC'] = None\n",
    "                    df['SURVEY_DATE'] = current_datetime\n",
    "                    df['VEHICLE'] = 'ISS'\n",
    "                    df['OPERATOR'] = 'ISS'\n",
    "                    df['USER_1_NAME'] = None\n",
    "                    df['USER_1'] = None\n",
    "                    df['USER_2_NAME'] = None\n",
    "                    df['USER_2'] = None\n",
    "                    df['USER_3_NAME'] = None\n",
    "                    df['USER_3'] = None\n",
    "                    df['LRP_FILE'] = f'LRP_{run_code}'\n",
    "                    df['LRP_RESET'] = 'N'\n",
    "                    df['LRP_START'] = 0\n",
    "                    df['CHAIN_INIT'] = 0\n",
    "                    df['CHAIN_START'] = 0\n",
    "                    df['CHAIN_END'] = round(df['numb_end'] * (df['max_chainage'] / df['pic_count'])).max()\n",
    "                    df['SECT_LEN'] = 0\n",
    "                    df['DIR'] = 'I'\n",
    "                    df['LANE'] = 1\n",
    "                    df['DEVICES'] = 'GPS-Geo-DR,LP_V3-LWP,LP_V3-RWP,TPL,Video'\n",
    "                    df['OTHERSIDE'] = True\n",
    "                    df['VERSION'] = '2.7.3.4/2.7.3.4'\n",
    "                    df['MEMO'] = None\n",
    "                    df['LENGTH'] = round(df['numb_end'] * (df['max_chainage'] / df['pic_count'])).max()\n",
    "\n",
    "                    df = df.astype({\n",
    "                        'SURVEY_DATE': 'datetime64[ns]',\n",
    "                        'LRP_START': 'int',\n",
    "                        'CHAIN_INIT': 'int',\n",
    "                        'CHAIN_START': 'int',\n",
    "                        'CHAIN_END': 'int',\n",
    "                        'SECT_LEN': 'int',\n",
    "                        'LANE': 'int',\n",
    "                        'OTHERSIDE': 'bool',\n",
    "                        'LENGTH': 'int'\n",
    "                    })\n",
    "                    \n",
    "                    return df\n",
    "\n",
    "                survey_header = mdb_survey_header(filtered_df)\n",
    "                survey_header = survey_header.groupby(['SURVEY_ID']).first().reset_index()\n",
    "                \n",
    "                selected_mdb_survey_header = [\n",
    "                    'SURVEY_ID', 'SURVEY_FILE', 'SURVEY_DESC', 'SURVEY_DATE', 'VEHICLE', 'OPERATOR', 'USER_1_NAME', 'USER_1', \n",
    "                    'USER_2_NAME', 'USER_2', 'USER_3_NAME', 'USER_3', 'LRP_FILE', 'LRP_RESET', 'LRP_START', 'CHAIN_INIT', \n",
    "                    'CHAIN_START','CHAIN_END', 'SECT_LEN', 'DIR', 'LANE', 'DEVICES', 'OTHERSIDE', 'VERSION', 'MEMO', 'LENGTH'\n",
    "                ]\n",
    "\n",
    "                selected_mdb_survey_header = [col for col in selected_mdb_survey_header if col in survey_header.columns]\n",
    "                mdb_survey_header_filename = os.path.join(mdb_folder_path, f'Survey_Header_{run_code}.csv')\n",
    "                survey_header[selected_mdb_survey_header].to_csv(mdb_survey_header_filename, index=False)\n",
    "                \n",
    "                def mdb_KeyCode_Raw(df):\n",
    "                    df['CHAINAGE_START'] = round(df['numb_start'] * (df['max_chainage'] / df['pic_count']))\n",
    "                    df['CHAINAGE_END'] = round(df['numb_end'] * (df['max_chainage'] / df['pic_count']))\n",
    "                    df['EVENT'] = df['event_name'].str[0].str.lower()\n",
    "                    df['SWITCH_GROUP'] = 'pave type.'\n",
    "                    df['EVENT_DESC'] = df['event_name'].str.lower()\n",
    "                    df['LATITUDE_START'] = df.groupby(['survey_code', 'linkid'])['rut_point_x'].transform('first')\n",
    "                    df['LATITUDE_END'] = df.groupby(['survey_code', 'linkid'])['rut_point_x'].transform('last')\n",
    "                    df['LONGITUDE_START'] = df.groupby(['survey_code', 'linkid'])['rut_point_y'].transform('first')\n",
    "                    df['LONGITUDE_END'] = df.groupby(['survey_code', 'linkid'])['rut_point_y'].transform('last')\n",
    "                    df['link_id'] = df['linkid']\n",
    "                    df['section_id'] = df['section_id']\n",
    "                    df['km_start'] = df['km_start']\n",
    "                    df['km_end'] = df['km_end']\n",
    "                    df['length'] = df['length']\n",
    "                    df['lane_no'] = df['linkid'].apply(lambda x: x[11:13])\n",
    "                    df['survey_date'] = df['date']\n",
    "                    \n",
    "                    return df\n",
    "\n",
    "                KeyCode_Raw = merged_df.groupby('survey_code', group_keys=False).apply(mdb_KeyCode_Raw).reset_index(drop=True)\n",
    "                KeyCode_Raw = KeyCode_Raw.groupby(['linkid', 'survey_date']).first().reset_index()\n",
    "                KeyCode_Raw = KeyCode_Raw[KeyCode_Raw['survey_code'] == folder_name]\n",
    "\n",
    "                selected_mdb_KeyCode_Raw = [\n",
    "                    'CHAINAGE_START', 'CHAINAGE_END', 'EVENT', 'SWITCH_GROUP', 'EVENT_DESC', 'LATITUDE_START', 'LATITUDE_END', \n",
    "                    'LONGITUDE_START', 'LONGITUDE_END', 'link_id', 'section_id', 'km_start', 'km_end', 'length', 'lane_no', \n",
    "                    'survey_date'\n",
    "                ]\n",
    "\n",
    "                selected_mdb_KeyCode_Raw = [col for col in selected_mdb_KeyCode_Raw if col in KeyCode_Raw.columns]\n",
    "                mdb_KeyCode_Raw_filename = os.path.join(mdb_folder_path, f'KeyCode_Raw_{run_code}.csv')\n",
    "                KeyCode_Raw[selected_mdb_KeyCode_Raw].sort_values(by=['lane_no', 'CHAINAGE_START', 'CHAINAGE_END'], ascending=[True, True, False]).to_csv(mdb_KeyCode_Raw_filename, index=False)    \n",
    "# insert .mdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main(final_df)\n",
    "    except Exception as e:\n",
    "        print(f\"‚õî Error in the main block: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‡πÅ‡∏Å‡πâ‡∏Å‡∏≤‡∏£ join ‡∏£‡∏π‡∏õ ‡πÉ‡∏´‡πâ chainage ‡∏Å‡∏±‡∏ö‡∏£‡∏π‡∏õ‡∏à‡∏ö‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‡πÅ‡∏Å‡πâ ‡∏Ñ‡∏≠‡∏•‡∏±‡∏ô‡∏°‡πå iri/rut ‡πÄ‡∏û‡∏¥‡πà‡∏° geom ‡πÄ‡∏û‡∏∑‡πà‡∏° gen iri ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÉ‡∏ä‡πâ code ‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏£‡∏π‡∏õ‡∏°‡∏≤‡∏Ñ‡∏£‡∏ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import fnmatch\n",
    "\n",
    "# Constants\n",
    "DIRECTORY_PATH = r\"D:\\xenomatix\\output\\survey_data_20240726\\Output\\20240726RUN02\"\n",
    "JPG_DIRECTORY_PATH = r\"D:\\xenomatix\\output\\survey_data_20240726\\PAVE\\20240726_2\\PAVE-0\"\n",
    "\n",
    "# Functions\n",
    "def get_jpg_filenames(directory):\n",
    "    jpg_dict = {}\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        jpg_files = [f for f in files if f.endswith('.jpg')]\n",
    "        if jpg_files:\n",
    "            folder_name = os.path.basename(os.path.dirname(root))\n",
    "            jpg_num = [int(fname.split('-')[-1].split('.jpg')[0]) for fname in jpg_files]\n",
    "    return jpg_num\n",
    "\n",
    "def process_csv_file(file_path, increment):\n",
    "    df = pd.read_csv(file_path, delimiter=';')\n",
    "    \n",
    "    df['event_start'] = 0\n",
    "    df['event_end'] = increment\n",
    "    \n",
    "    event_start = 0\n",
    "    event_end = increment\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        df.at[i, 'event_start'] = event_start\n",
    "        df.at[i, 'event_end'] = event_end\n",
    "        event_start = event_end\n",
    "        event_end += increment\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_interval_rows(df, start_col='event_start', end_col='event_end', interval_size=5):\n",
    "    new_rows = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        start = row[start_col]\n",
    "        end = row[end_col]\n",
    "        \n",
    "        while start < end:\n",
    "            new_row = row.copy()\n",
    "            new_row[start_col] = start\n",
    "            new_row[end_col] = start + interval_size\n",
    "            new_rows.append(new_row)\n",
    "            start += interval_size\n",
    "    \n",
    "    return pd.DataFrame(new_rows)\n",
    "\n",
    "def left_join_dataframes(df_rutting, df_iri):\n",
    "    return pd.merge(df_rutting, df_iri, how='left', on=['event_start', 'event_end'], suffixes=('_rutting', '_iri'))\n",
    "\n",
    "def add_frame_num_to_joined_df(joined_df, derived_values, frame_numbers):\n",
    "    joined_df['frame_num_ch'] = pd.NA\n",
    "    joined_df['frame_num'] = pd.NA\n",
    "    \n",
    "    derived_to_frame_mapping = pd.DataFrame({\n",
    "        'frame_num_ch': derived_values,\n",
    "        'frame_num': frame_numbers\n",
    "    })\n",
    "    \n",
    "    for i, frame_num_ch in enumerate(derived_values):\n",
    "        mask = (joined_df['event_start'] <= frame_num_ch) & (joined_df['event_end'] > frame_num_ch)\n",
    "        joined_df.loc[mask, 'frame_num_ch'] = frame_num_ch\n",
    "        joined_df.loc[mask, 'frame_num'] = frame_numbers[i]\n",
    "    \n",
    "    return joined_df\n",
    "\n",
    "# Main\n",
    "jpg_filenames = get_jpg_filenames(JPG_DIRECTORY_PATH)\n",
    "frame_numbers = extract_numeric_from_filenames(jpg_filenames)\n",
    "\n",
    "dataframes = {}\n",
    "\n",
    "for filename in os.listdir(DIRECTORY_PATH):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(DIRECTORY_PATH, filename)\n",
    "        increment = 20 if fnmatch.fnmatch(filename, '*xw_iri_qgis*') else 5\n",
    "        df = process_csv_file(file_path, increment)\n",
    "        \n",
    "        # Create new rows based on intervals\n",
    "        new_df = create_interval_rows(df, interval_size=5)\n",
    "        \n",
    "        # Store the new DataFrame in the dictionary with the filename as the key\n",
    "        dataframes[filename] = new_df\n",
    "\n",
    "print(\"Processed and updated CSV files into DataFrames.\")\n",
    "\n",
    "# Perform the left join and store the result\n",
    "joined_dataframes = {}\n",
    "\n",
    "for rutting_file in dataframes:\n",
    "    if fnmatch.fnmatch(rutting_file, '*xw_rutting*'):\n",
    "        for iri_file in dataframes:\n",
    "            if fnmatch.fnmatch(iri_file, '*xw_iri_qgis*'):\n",
    "                joined_df = left_join_dataframes(dataframes[rutting_file], dataframes[iri_file])\n",
    "                \n",
    "                max_event_start = joined_df['event_start'].max()\n",
    "                \n",
    "                derived_values = [round((max_event_start * num) / max(frame_numbers)) for num in frame_numbers]\n",
    "                print('max_event_end = ', max_event_start)\n",
    "                print('frame_numbers = ', max(frame_numbers))\n",
    "                print(\"Derived Values:\", derived_values)\n",
    "                print(\"Count of derived_values:\", len(derived_values))\n",
    "                print(\"Frame Numbers:\", frame_numbers)\n",
    "                print(\"Count of frame_numbers:\", len(frame_numbers))\n",
    "                \n",
    "                joined_df = add_frame_num_to_joined_df(joined_df, derived_values, frame_numbers)\n",
    "               \n",
    "                joined_dataframes[f\"{rutting_file}_{iri_file}\"] = joined_df\n",
    "\n",
    "print(\"Performed the left join and stored the result.\")\n",
    "\n",
    "output_dir = r'D:\\xenomatix\\output'\n",
    "iri_dataframes, rutting_dataframes = process_csv_files(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>iri left (m/km)</th>\n",
       "      <th>iri Std left (m/km)</th>\n",
       "      <th>iri right (m/km)</th>\n",
       "      <th>iri Std right (m/km)</th>\n",
       "      <th>worst iri (m/km)</th>\n",
       "      <th>iri difference (m/km)</th>\n",
       "      <th>geometry</th>\n",
       "      <th>event_start</th>\n",
       "      <th>event_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>3.865407</td>\n",
       "      <td>2.804227</td>\n",
       "      <td>3.482863</td>\n",
       "      <td>2.631087</td>\n",
       "      <td>3.865407</td>\n",
       "      <td>0.382544</td>\n",
       "      <td>LINESTRING(100.4438015813279 13.85304028353679...</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>3.547611</td>\n",
       "      <td>2.251651</td>\n",
       "      <td>3.629724</td>\n",
       "      <td>2.853411</td>\n",
       "      <td>3.629724</td>\n",
       "      <td>0.082113</td>\n",
       "      <td>LINESTRING(100.4438338646643 13.85286229758905...</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>3.097305</td>\n",
       "      <td>2.065051</td>\n",
       "      <td>3.022079</td>\n",
       "      <td>2.184983</td>\n",
       "      <td>3.097305</td>\n",
       "      <td>0.075226</td>\n",
       "      <td>LINESTRING(100.44385608110896 13.8526828659645...</td>\n",
       "      <td>40</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>4.105208</td>\n",
       "      <td>3.417165</td>\n",
       "      <td>3.377205</td>\n",
       "      <td>2.298076</td>\n",
       "      <td>4.105208</td>\n",
       "      <td>0.728003</td>\n",
       "      <td>LINESTRING(100.44387122090556 13.8525027087672...</td>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>2.856759</td>\n",
       "      <td>1.884442</td>\n",
       "      <td>2.991038</td>\n",
       "      <td>2.360168</td>\n",
       "      <td>2.991038</td>\n",
       "      <td>0.134279</td>\n",
       "      <td>LINESTRING(100.44388152103689 13.8523222191993...</td>\n",
       "      <td>80</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>5.569864</td>\n",
       "      <td>4.323723</td>\n",
       "      <td>4.514422</td>\n",
       "      <td>3.205883</td>\n",
       "      <td>5.569864</td>\n",
       "      <td>1.055441</td>\n",
       "      <td>LINESTRING(100.48977019652955 13.8559590270556...</td>\n",
       "      <td>6060</td>\n",
       "      <td>6080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>6.403876</td>\n",
       "      <td>3.835795</td>\n",
       "      <td>5.234916</td>\n",
       "      <td>3.866694</td>\n",
       "      <td>6.403876</td>\n",
       "      <td>1.168960</td>\n",
       "      <td>LINESTRING(100.48993714618199 13.8560367764371...</td>\n",
       "      <td>6080</td>\n",
       "      <td>6100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>6.735491</td>\n",
       "      <td>4.039852</td>\n",
       "      <td>6.620786</td>\n",
       "      <td>6.458464</td>\n",
       "      <td>6.735491</td>\n",
       "      <td>0.114704</td>\n",
       "      <td>LINESTRING(100.49011085619544 13.8560987970925...</td>\n",
       "      <td>6100</td>\n",
       "      <td>6120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>4.782251</td>\n",
       "      <td>4.015040</td>\n",
       "      <td>5.930027</td>\n",
       "      <td>4.991943</td>\n",
       "      <td>5.930027</td>\n",
       "      <td>1.147776</td>\n",
       "      <td>LINESTRING(100.4902900220181 13.85614367654372...</td>\n",
       "      <td>6120</td>\n",
       "      <td>6140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>26/07/2024</td>\n",
       "      <td>4.743412</td>\n",
       "      <td>4.057893</td>\n",
       "      <td>7.756645</td>\n",
       "      <td>6.429222</td>\n",
       "      <td>7.756645</td>\n",
       "      <td>3.013233</td>\n",
       "      <td>LINESTRING(100.49047163785643 13.8561780408464...</td>\n",
       "      <td>6140</td>\n",
       "      <td>6160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>308 rows √ó 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date   iri left (m/km)   iri Std left (m/km)   iri right (m/km)  \\\n",
       "0    26/07/2024          3.865407              2.804227           3.482863   \n",
       "1    26/07/2024          3.547611              2.251651           3.629724   \n",
       "2    26/07/2024          3.097305              2.065051           3.022079   \n",
       "3    26/07/2024          4.105208              3.417165           3.377205   \n",
       "4    26/07/2024          2.856759              1.884442           2.991038   \n",
       "..          ...               ...                   ...                ...   \n",
       "303  26/07/2024          5.569864              4.323723           4.514422   \n",
       "304  26/07/2024          6.403876              3.835795           5.234916   \n",
       "305  26/07/2024          6.735491              4.039852           6.620786   \n",
       "306  26/07/2024          4.782251              4.015040           5.930027   \n",
       "307  26/07/2024          4.743412              4.057893           7.756645   \n",
       "\n",
       "      iri Std right (m/km)   worst iri (m/km)   iri difference (m/km)  \\\n",
       "0                 2.631087           3.865407                0.382544   \n",
       "1                 2.853411           3.629724                0.082113   \n",
       "2                 2.184983           3.097305                0.075226   \n",
       "3                 2.298076           4.105208                0.728003   \n",
       "4                 2.360168           2.991038                0.134279   \n",
       "..                     ...                ...                     ...   \n",
       "303               3.205883           5.569864                1.055441   \n",
       "304               3.866694           6.403876                1.168960   \n",
       "305               6.458464           6.735491                0.114704   \n",
       "306               4.991943           5.930027                1.147776   \n",
       "307               6.429222           7.756645                3.013233   \n",
       "\n",
       "                                              geometry  event_start  event_end  \n",
       "0    LINESTRING(100.4438015813279 13.85304028353679...            0         20  \n",
       "1    LINESTRING(100.4438338646643 13.85286229758905...           20         40  \n",
       "2    LINESTRING(100.44385608110896 13.8526828659645...           40         60  \n",
       "3    LINESTRING(100.44387122090556 13.8525027087672...           60         80  \n",
       "4    LINESTRING(100.44388152103689 13.8523222191993...           80        100  \n",
       "..                                                 ...          ...        ...  \n",
       "303  LINESTRING(100.48977019652955 13.8559590270556...         6060       6080  \n",
       "304  LINESTRING(100.48993714618199 13.8560367764371...         6080       6100  \n",
       "305  LINESTRING(100.49011085619544 13.8560987970925...         6100       6120  \n",
       "306  LINESTRING(100.4902900220181 13.85614367654372...         6120       6140  \n",
       "307  LINESTRING(100.49047163785643 13.8561780408464...         6140       6160  \n",
       "\n",
       "[308 rows x 10 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
